{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c839315",
   "metadata": {},
   "source": [
    "## 10. Tips and Advanced Usage\n",
    "\n",
    "### Model Selection\n",
    "- **CLIP (ViT-B/32)**: Good balance of speed and quality. Excellent for general-purpose tasks.\n",
    "- **AlignCLIP**: Improved alignment variant. Better at fine-grained distinctions.\n",
    "- **CLOOB**: Uses optimal transport for better alignment. Slower but potentially better quality.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "- **Learning Rate**: Typically 1e-6 to 1e-4. Lower rates for heavier models.\n",
    "- **Batch Size**: 32-256 depending on GPU memory.\n",
    "- **Temperature**: Controls contrastive sharpness. 0.07 is default. Lower = sharper, Higher = softer.\n",
    "- **Weight Decay**: L2 regularization strength. Usually 0.01-0.1.\n",
    "\n",
    "### Tips for Better Results\n",
    "1. Use more diverse training data\n",
    "2. Increase training epochs when using large datasets\n",
    "3. Use gradient accumulation for effective larger batch sizes\n",
    "4. Consider data augmentation strategies\n",
    "5. Fine-tune on task-specific data for better performance\n",
    "\n",
    "### Next Steps\n",
    "- Export finetuned models for production\n",
    "- Evaluate on downstream tasks (retrieval, classification)\n",
    "- Combine ensemble predictions from all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c148786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_texts(model: nn.Module, texts: List[str], device: torch.device) -> torch.Tensor:\n",
    "    \"\"\"Encode a list of texts to embeddings.\"\"\"\n",
    "    tokens = torch.cat([clip.tokenize(text) for text in texts])\n",
    "    tokens = tokens.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_features = model.model.encode_text(tokens)\n",
    "        text_features = torch.nn.functional.normalize(text_features, dim=-1)\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "\n",
    "def compute_similarity(model: nn.Module, text: str, device: torch.device) -> float:\n",
    "    \"\"\"Compute similarity between text and sample texts.\"\"\"\n",
    "    text_features = encode_texts(model, [text], device)\n",
    "    \n",
    "    # Sample texts from dataset\n",
    "    sample_texts = val_data[:5]\n",
    "    sample_text_list = [s['text'] for s in sample_texts]\n",
    "    sample_features = encode_texts(model, sample_text_list, device)\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = text_features @ sample_features.T\n",
    "    return similarities.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "# Test inference with CLIP\n",
    "test_text = \"A beautiful sunset over mountains\"\n",
    "logger.info(f\"\\nTesting inference with text: '{test_text}'\")\n",
    "\n",
    "clip_model.eval()\n",
    "similarities = compute_similarity(clip_model, test_text, device)\n",
    "logger.info(\"Text Similarities (CLIP):\")\n",
    "for i, (text, sim) in enumerate(zip([s['text'] for s in val_data[:5]], similarities)):\n",
    "    logger.info(f\"  {i+1}. {sim:.4f} - {text[:50]}...\")\n",
    "\n",
    "print(\"\\nInference test completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ca0fa",
   "metadata": {},
   "source": [
    "## 9. Inference and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fa0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# CLIP losses\n",
    "axes[0].plot(clip_trainer.train_losses, label='Train', marker='o')\n",
    "axes[0].plot(clip_trainer.val_losses, label='Val', marker='s')\n",
    "axes[0].set_title('CLIP')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AlignCLIP losses\n",
    "axes[1].plot(align_trainer.train_losses, label='Train', marker='o')\n",
    "axes[1].plot(align_trainer.val_losses, label='Val', marker='s')\n",
    "axes[1].set_title('AlignCLIP')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# CLOOB losses (if available)\n",
    "if 'cloob_trainer' in locals():\n",
    "    axes[2].plot(cloob_trainer.train_losses, label='Train', marker='o')\n",
    "    axes[2].plot(cloob_trainer.val_losses, label='Val', marker='s')\n",
    "    axes[2].set_title('CLOOB')\n",
    "    axes[2].set_xlabel('Epoch')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'CLOOB Not Available', ha='center', va='center')\n",
    "    axes[2].set_title('CLOOB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"CLIP      - Best Val Loss: {clip_trainer.best_val_loss:.6f}\")\n",
    "print(f\"AlignCLIP - Best Val Loss: {align_trainer.best_val_loss:.6f}\")\n",
    "if 'cloob_trainer' in locals():\n",
    "    print(f\"CLOOB     - Best Val Loss: {cloob_trainer.best_val_loss:.6f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e241695",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3319c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CLOOB if initialization succeeded\n",
    "try:\n",
    "    cloob_trainer.train(num_epochs=config['num_epochs'])\n",
    "    cloob_trainer.plot_losses()\n",
    "except NameError:\n",
    "    logger.info(\"CLOOB trainer not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLOOB model\n",
    "logger.info(\"Initializing CLOOB model...\")\n",
    "try:\n",
    "    cloob_model = CLOOBModel(device=str(device))\n",
    "    \n",
    "    # Create datasets with CLOOB preprocessing\n",
    "    train_dataset_cloob = CLIPDataset(train_data, preprocess=cloob_model.preprocess, model_name=\"cloob\")\n",
    "    val_dataset_cloob = CLIPDataset(val_data, preprocess=cloob_model.preprocess, model_name=\"cloob\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader_cloob = DataLoader(\n",
    "        train_dataset_cloob,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader_cloob = DataLoader(\n",
    "        val_dataset_cloob,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    # Setup loss and optimizer for CLOOB\n",
    "    cloob_criterion = CLIPContrastiveLoss(temperature=config['temperature'])\n",
    "    \n",
    "    cloob_optimizer = optim.AdamW(\n",
    "        cloob_model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    cloob_trainer = CLIPTrainer(\n",
    "        model=cloob_model,\n",
    "        train_loader=train_loader_cloob,\n",
    "        val_loader=val_loader_cloob,\n",
    "        criterion=cloob_criterion,\n",
    "        optimizer=cloob_optimizer,\n",
    "        device=device,\n",
    "        config=config,\n",
    "        model_name=\"cloob\"\n",
    "    )\n",
    "    \n",
    "    logger.info(\"CLOOB trainer initialized\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to initialize CLOOB: {e}\")\n",
    "    logger.info(\"Skipping CLOOB training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abf06e",
   "metadata": {},
   "source": [
    "## 7. Finetune CLOOB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train AlignCLIP\n",
    "align_trainer.train(num_epochs=config['num_epochs'])\n",
    "align_trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4b5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AlignCLIP model\n",
    "logger.info(\"Initializing AlignCLIP model...\")\n",
    "alignclip_model = AlignCLIPFinetunableModel(device=str(device))\n",
    "\n",
    "# Create datasets with AlignCLIP preprocessing\n",
    "train_dataset_align = CLIPDataset(train_data, preprocess=alignclip_model.preprocess, model_name=\"alignclip\")\n",
    "val_dataset_align = CLIPDataset(val_data, preprocess=alignclip_model.preprocess, model_name=\"alignclip\")\n",
    "\n",
    "# Create data loaders (reuse same batch configuration)\n",
    "train_loader_align = DataLoader(\n",
    "    train_dataset_align,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "val_loader_align = DataLoader(\n",
    "    val_dataset_align,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "# Setup loss and optimizer for AlignCLIP\n",
    "align_criterion = CLIPContrastiveLoss(temperature=config['temperature'])\n",
    "\n",
    "align_optimizer = optim.AdamW(\n",
    "    alignclip_model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "align_trainer = CLIPTrainer(\n",
    "    model=alignclip_model,\n",
    "    train_loader=train_loader_align,\n",
    "    val_loader=val_loader_align,\n",
    "    criterion=align_criterion,\n",
    "    optimizer=align_optimizer,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    model_name=\"alignclip\"\n",
    ")\n",
    "\n",
    "logger.info(\"AlignCLIP trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757cab67",
   "metadata": {},
   "source": [
    "## 6. Finetune AlignCLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818031ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CLIP\n",
    "clip_trainer.train(num_epochs=config['num_epochs'])\n",
    "clip_trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e81b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss and optimizer for CLIP\n",
    "clip_criterion = CLIPContrastiveLoss(temperature=config['temperature'])\n",
    "\n",
    "# Only finetune visual and text heads (not backbone for efficiency)\n",
    "clip_optimizer = optim.AdamW(\n",
    "    clip_model.parameters(),\n",
    "    lr=config['learning_rate'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "clip_trainer = CLIPTrainer(\n",
    "    model=clip_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=clip_criterion,\n",
    "    optimizer=clip_optimizer,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    model_name=\"clip\"\n",
    ")\n",
    "\n",
    "logger.info(\"CLIP trainer initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CLIP model\n",
    "logger.info(\"Initializing CLIP model...\")\n",
    "clip_model = CLIPFinetunableModel(device=str(device))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CLIPDataset(train_data, preprocess=clip_model.preprocess, model_name=\"clip\")\n",
    "val_dataset = CLIPDataset(val_data, preprocess=clip_model.preprocess, model_name=\"clip\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 to avoid multiprocessing issues in Jupyter\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "logger.info(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14523fd",
   "metadata": {},
   "source": [
    "## 5. Finetune CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f02325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPTrainer:\n",
    "    \"\"\"Trainer for CLIP-style models.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: torch.device,\n",
    "        config: Dict,\n",
    "        model_name: str = \"clip\"\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Training tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "    def train_epoch(self) -> float:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for batch_idx, (images, text_tokens, _) in enumerate(pbar):\n",
    "            images = images.to(self.device)\n",
    "            text_tokens = text_tokens.to(self.device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                image_features, text_features = self.model(images, text_tokens)\n",
    "                loss = self.criterion(image_features, text_features)\n",
    "            \n",
    "            # Backward pass\n",
    "            self.scaler.scale(loss).backward()\n",
    "            \n",
    "            # Gradient accumulation\n",
    "            if (batch_idx + 1) % self.config['grad_accumulation_steps'] == 0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(self.train_loader)\n",
    "        self.train_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def validate(self) -> float:\n",
    "        \"\"\"Validate model.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc=\"Validating\")\n",
    "            for images, text_tokens, _ in pbar:\n",
    "                images = images.to(self.device)\n",
    "                text_tokens = text_tokens.to(self.device)\n",
    "                \n",
    "                image_features, text_features = self.model(images, text_tokens)\n",
    "                loss = self.criterion(image_features, text_features)\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        self.val_losses.append(avg_loss)\n",
    "        return avg_loss\n",
    "    \n",
    "    def train(self, num_epochs: int):\n",
    "        \"\"\"Train model for specified epochs.\"\"\"\n",
    "        logger.info(f\"Starting training on {self.device} for {num_epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = self.train_epoch()\n",
    "            logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = self.validate()\n",
    "            logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.save_checkpoint(f\"best_{self.model_name}_model.pt\")\n",
    "                logger.info(f\"âœ“ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "    \n",
    "    def save_checkpoint(self, filename: str):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'config': self.config,\n",
    "        }\n",
    "        torch.save(checkpoint, filename)\n",
    "        logger.info(f\"Checkpoint saved: {filename}\")\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and validation losses.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.train_losses, label='Train Loss', marker='o')\n",
    "        plt.plot(self.val_losses, label='Val Loss', marker='s')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'{self.model_name.upper()} Training Curves')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b254bb",
   "metadata": {},
   "source": [
    "## 4. Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df84b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 5,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 0.1,\n",
    "    'warmup_steps': 100,\n",
    "    'num_workers': 4,\n",
    "    'grad_accumulation_steps': 1,\n",
    "    'temperature': 0.07,\n",
    "    'model_name': 'clip',  # Options: 'clip', 'alignclip', 'cloob'\n",
    "}\n",
    "\n",
    "logger.info(f\"Training config: {json.dumps(config, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfef636",
   "metadata": {},
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPFinetunableModel(nn.Module):\n",
    "    \"\"\"Wrapper for CLIP model for finetuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = None):\n",
    "        super().__init__()\n",
    "        # Import and use actual CLIPModel from codebase\n",
    "        from Models.clipModel import CLIPModel\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = CLIPModel(device=self.device)\n",
    "        self.preprocess = self.model.preprocess\n",
    "        \n",
    "    def forward(self, images: torch.Tensor, text_tokens: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (image_features, text_features)\n",
    "        \"\"\"\n",
    "        image_features = self.model.encode_image_tensors(images, requires_grad=True)\n",
    "        text_features = self.model.encode_text_tokens(text_tokens, requires_grad=True)\n",
    "        \n",
    "        return image_features, text_features\n",
    "\n",
    "\n",
    "class AlignCLIPFinetunableModel(nn.Module):\n",
    "    \"\"\"Wrapper for AlignCLIP model for finetuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = None):\n",
    "        super().__init__()\n",
    "        # Import and use actual AlignCLIPModel from codebase\n",
    "        from Models.alignClipModel import AlignCLIPModel\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = AlignCLIPModel(device=self.device)\n",
    "        self.preprocess = self.model.preprocess\n",
    "        \n",
    "    def forward(self, images: torch.Tensor, text_tokens: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        image_features = self.model.encode_image_tensors(images, requires_grad=True)\n",
    "        text_features = self.model.encode_text_tokens(text_tokens, requires_grad=True)\n",
    "        return image_features, text_features\n",
    "\n",
    "\n",
    "class CLOOBFinetunableModel(nn.Module):\n",
    "    \"\"\"Wrapper for CLOOB model for finetuning.\"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = None):\n",
    "        super().__init__()\n",
    "        # Import and use actual CLOOBModel from codebase\n",
    "        from Models.cloobModel import CLOOBModel\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = CLOOBModel(device=self.device)\n",
    "        # CLOOB uses CLIP preprocessing\n",
    "        self.preprocess = clip.load(\"ViT-B/32\", device=self.device)[1]\n",
    "    \n",
    "    def forward(self, images: torch.Tensor, text_tokens: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        image_features = self.model.encode_image_tensors(images, requires_grad=True)\n",
    "        text_features = self.model.encode_text_tokens(text_tokens, requires_grad=True)\n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9650a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss for CLIP-style models.\n",
    "    Aligns image and text embeddings in shared space.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, temperature: float = 0.07):\n",
    "        \"\"\"\n",
    "        Initialize loss.\n",
    "        \n",
    "        Args:\n",
    "            temperature: Temperature parameter for scaling logits\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, image_features: torch.Tensor, text_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute contrastive loss.\n",
    "        \n",
    "        Args:\n",
    "            image_features: Image embeddings [batch_size, embedding_dim]\n",
    "            text_features: Text embeddings [batch_size, embedding_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        # Normalize features\n",
    "        image_features = torch.nn.functional.normalize(image_features, dim=-1)\n",
    "        text_features = torch.nn.functional.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute logits\n",
    "        logits_per_image = image_features @ text_features.T / self.temperature\n",
    "        logits_per_text = text_features @ image_features.T / self.temperature\n",
    "        \n",
    "        # Create labels (diagonal elements are positive pairs)\n",
    "        batch_size = image_features.shape[0]\n",
    "        labels = torch.arange(batch_size, device=image_features.device)\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss_img = torch.nn.functional.cross_entropy(logits_per_image, labels)\n",
    "        loss_txt = torch.nn.functional.cross_entropy(logits_per_text, labels)\n",
    "        \n",
    "        loss = (loss_img + loss_txt) / 2\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723aa83",
   "metadata": {},
   "source": [
    "## 2. Define Loss Functions and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e663e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train/val\n",
    "train_ratio = 0.8\n",
    "split_idx = int(len(dataset) * train_ratio)\n",
    "train_data = dataset[:split_idx]\n",
    "val_data = dataset[split_idx:]\n",
    "\n",
    "logger.info(f\"Train samples: {len(train_data)}, Val samples: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for CLIP-style models.\n",
    "    Handles image-text pairs with preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], preprocess=None, model_name: str = \"clip\"):\n",
    "        \"\"\"\n",
    "        Initialize dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: List of dicts with 'image_path' and 'text' keys\n",
    "            preprocess: Image preprocessing function\n",
    "            model_name: One of 'clip', 'alignclip', 'cloob'\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.preprocess = preprocess\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Initialize tokenizer based on model\n",
    "        if model_name in [\"clip\", \"alignclip\"]:\n",
    "            self.context_length = 77\n",
    "        elif model_name == \"cloob\":\n",
    "            self.context_length = 77\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, str]:\n",
    "        \"\"\"\n",
    "        Get a single sample.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (image_tensor, text_tokens, text)\n",
    "        \"\"\"\n",
    "        sample = self.data[idx]\n",
    "        text = sample.get('text', '')\n",
    "        \n",
    "        # Handle image\n",
    "        if sample.get('image_path') and os.path.exists(sample['image_path']):\n",
    "            try:\n",
    "                image = Image.open(sample['image_path']).convert('RGB')\n",
    "                if self.preprocess:\n",
    "                    image = self.preprocess(image)\n",
    "                else:\n",
    "                    # Default preprocessing\n",
    "                    image = torch.zeros(3, 224, 224)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error loading image {sample['image_path']}: {e}\")\n",
    "                image = torch.zeros(3, 224, 224)\n",
    "        else:\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        \n",
    "        # Tokenize text\n",
    "        if self.model_name in [\"clip\", \"alignclip\"]:\n",
    "            text_tokens = clip.tokenize(text, context_length=self.context_length)[0]\n",
    "        else:\n",
    "            text_tokens = clip.tokenize(text, context_length=self.context_length)[0]\n",
    "        \n",
    "        return image, text_tokens, text\n",
    "\n",
    "\n",
    "# Load sample dataset using DatasetLoader\n",
    "logger.info(\"Loading sample dataset...\")\n",
    "dataset = DatasetLoader.load_laion_sample()\n",
    "logger.info(f\"Loaded {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0444c46a",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add workspace to path\n",
    "workspace_path = Path.cwd()\n",
    "sys.path.insert(0, str(workspace_path))\n",
    "\n",
    "# Import custom dataset loader\n",
    "from datasetLoader import DatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ced44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "import clip\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c1e7a",
   "metadata": {},
   "source": [
    "# Multimodal Model Finetuning Notebook\n",
    "## CLIP, AlignCLIP, and CLOOB Finetuning\n",
    "\n",
    "This notebook provides a comprehensive framework for finetuning three multimodal models:\n",
    "- **CLIP**: OpenAI's Contrastive Language-Image Pre-training\n",
    "- **AlignCLIP**: Improved alignment variant of CLIP\n",
    "- **CLOOB**: Contrastive Learning with Optimal Transport for Image-Text\n",
    "\n",
    "All models use contrastive learning to align image and text embeddings in a shared space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3867f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
