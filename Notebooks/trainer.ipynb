{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialization and Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18056b98",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "IS_COLAB = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086dad0c",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:   \n",
        "    import os\n",
        "\n",
        "    github_token = \"\"\n",
        "    github_username = \"\" # Replace with your GitHub username\n",
        "    repository_url = f\"https://{github_username}:{github_token}@github.com/neskech/Multimodal-2025.git\"\n",
        "\n",
        "    !git clone {repository_url}\n",
        "    \n",
        "\n",
        "    %cd Multimodal-2025\n",
        "    !git submodule update --init --recursive\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6c7a23",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reqs = \"\"\"\n",
        "torch>=1.9.0\n",
        "torchvision>=0.10.0\n",
        "numpy>=1.21.0\n",
        "scipy>=1.7.0\n",
        "scikit-learn>=1.0.0\n",
        "matplotlib>=3.4.0\n",
        "seaborn>=0.11.0\n",
        "Pillow>=8.3.0\n",
        "clip @ git+https://github.com/openai/CLIP.git@dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
        "power-spherical @ git+https://github.com/nicola-decao/power_spherical.git@3d4619a9d6c01bc9b427533d386271a233e304cd\n",
        "dotenv\n",
        "\"\"\"\n",
        "with open(\"/root/Multimodal-2025/requirements.txt\", \"w\") as f:\n",
        "    f.write(reqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5199b9",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%uv pip install -r requirements.txt\n",
        "%uv pip install wandb\n",
        "%uv pip install webdataset\n",
        "%uv pip install datasets\n",
        "!cd Multimodal-2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cac9fdd",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB: # colab does not seem to support these\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "    %reload_ext autoreload\n",
        "\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import wandb\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from typing import Literal, Union\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "sys.path.append(\"Multimodal-2025\")\n",
        "from Datasets.coco import CocoDataset\n",
        "from Datasets.cc12m import CC12mDataset\n",
        "from Datasets.cood import CoodDataset\n",
        "from Datasets.laion import LaionDataset\n",
        "from Models.variationalClip import VariationalCLIPModel\n",
        "from losses.vclipLoss import VClipLoss\n",
        "from power_spherical import PowerSpherical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a358e42",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Type aliases for compatibility with older Python versions\n",
        "type Dataset = Literal['COCO', 'COOD', 'CC12M', 'LAION']\n",
        "DATASET: Dataset = 'COCO'\n",
        "\n",
        "DEVICE = None\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'\n",
        "elif torch.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "else:\n",
        "    DEVICE = 'cpu'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get WANDB Key (Use a .env file to store the key)\n",
        "load_dotenv()\n",
        "WANDB_API_KEY = os.environ.get('WANDB_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e51fabe",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'NUM_EPOCHS': 20,\n",
        "    'BATCH_SIZE': 128,\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'WEIGHT_DECAY': 1e-8,\n",
        "\n",
        "    # Scheduler parameters\n",
        "    'WARMUP_EPOCHS': 2,\n",
        "    'DECAY_EPOCHS': 30,\n",
        "\n",
        "    # To avoid gradient explosion. Set to 1 to disable\n",
        "    'GRAD_ACCUMULATION_STEPS': 1,\n",
        "    # Clip gradients to avoid explosion\n",
        "    'CLIP_GRADIENTS': False,\n",
        "    'EMPTY_CACHE_AFTER_BATCH': False,\n",
        "\n",
        "    # False to disable preloaded weights\n",
        "    'LOAD_PRETRAINED_WEIGHTS': False,\n",
        "\n",
        "    'KL_WEIGHT': 100,\n",
        "    'NUM_EPOCHS_TO_FULL_KL': 5,\n",
        "\n",
        "    'DATA_DIR': '/mnt/content/Data',\n",
        "    'TRAIN_RATIO': 0.9,\n",
        "    'TOTAL_DATAPOINTS': 50_000,\n",
        "\n",
        "    'USE_WANDB': True,\n",
        "    'WANDB_RUN_NAME': f'VCLIP_Train_On_{DATASET}_{50_000}',\n",
        "    'WANDB_PREVIOUS_RUN_ID': None,  # set to None if not resuming\n",
        "    'WANDB_PROJECT_NAME': 'multimodal_2025',\n",
        "\n",
        "    'FREEZE_BACKBONE': True,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82ba62c6",
      "metadata": {},
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7a676d6",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5fda5e2",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_train = int(CONFIG['TOTAL_DATAPOINTS'] * CONFIG['TRAIN_RATIO'])\n",
        "num_val = int(CONFIG['TOTAL_DATAPOINTS'] * (1.0 - CONFIG['TRAIN_RATIO']))\n",
        "\n",
        "if DATASET == 'COCO':\n",
        "    train = CocoDataset(\n",
        "        data_dir=\"/mnt/content/Data\",\n",
        "        split='train2017',\n",
        "        tokenize=True,\n",
        "        max_samples=num_train\n",
        "    )\n",
        "    val = CocoDataset(\n",
        "        data_dir=\"/mnt/content/Data\",\n",
        "        split='val2017',\n",
        "        tokenize=True,\n",
        "        max_samples=num_val\n",
        "    )\n",
        "    collate_fn = CocoDataset.collate_function\n",
        "elif DATASET == 'COOD':\n",
        "    CoodDataset.download(data_dir=CONFIG['DATA_DIR'])\n",
        "    all_data = CoodDataset(\n",
        "        data_dir=CONFIG['DATA_DIR'],\n",
        "        tokenize=True,\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
        "    )\n",
        "    train = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(0, num_train)\n",
        "    )\n",
        "    val = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
        "    )\n",
        "    collate_fn = CoodDataset.collate_function\n",
        "elif DATASET == 'LAION':\n",
        "    LaionDataset.download(\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
        "    all_data = LaionDataset(\n",
        "        data_dir=CONFIG['DATA_DIR'],\n",
        "        tokenize=True,\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
        "    )\n",
        "    train = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(0, num_train)\n",
        "    )\n",
        "    val = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
        "    )\n",
        "    collate_fn = LaionDataset.collate_function\n",
        "elif DATASET == 'CC12M':\n",
        "    CC12mDataset.download(\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
        "    all_data = CC12mDataset(\n",
        "        data_dir=CONFIG['DATA_DIR'],\n",
        "        tokenize=True,\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
        "    )\n",
        "    train = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(0, num_train)\n",
        "    )\n",
        "    val = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
        "    )\n",
        "    collate_fn = CC12mDataset.collate_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65ffffbc",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train,\n",
        "    CONFIG['BATCH_SIZE'],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=DEVICE == 'cuda',\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val,\n",
        "    CONFIG['BATCH_SIZE'],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=DEVICE == 'cuda',\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd349f2",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "logger.info(\n",
        "    f\"Training on {len(train)} samples, validating on {len(val)} samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cf21929",
      "metadata": {},
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b085aad",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "wmodel = model.float().to(DEVICE).freeze_backbone(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8bd3f6a",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loss = VClipLoss(kl_weight=CONFIG['KL_WEIGHT'],\n",
        "                 use_mean_only=False, label_smoothing=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "344829b2",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['LEARNING_RATE'],\n",
        "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
        ")\n",
        "\n",
        "# Warmup scheduler: Linear increase from 0 to target_lr\n",
        "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=0.01, end_factor=1.0, total_iters=CONFIG['WARMUP_EPOCHS'])\n",
        "\n",
        "# Decay scheduler: Cosine annealing after warmup\n",
        "decay_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=CONFIG['DECAY_EPOCHS'])\n",
        "\n",
        "# Combine them using SequentialLR\n",
        "# The schedulers will be applied sequentially\n",
        "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[\n",
        "                                                  warmup_scheduler, decay_scheduler], milestones=[CONFIG['WARMUP_EPOCHS']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bc8f86e",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def kl_schedule(epoch: int) -> float:\n",
        "    \"\"\"Returns the KL weight for the given epoch based on a linear schedule.\"\"\"\n",
        "    epoch = epoch + 1  # Epochs are 0-indexed\n",
        "    if epoch < CONFIG['NUM_EPOCHS_TO_FULL_KL']:\n",
        "        return 0\n",
        "    if epoch >= 2 * CONFIG['NUM_EPOCHS_TO_FULL_KL']:\n",
        "        return CONFIG['KL_WEIGHT']\n",
        "    else:\n",
        "        return CONFIG['KL_WEIGHT'] * (epoch / (2 * CONFIG['NUM_EPOCHS_TO_FULL_KL']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6be7b7ce",
      "metadata": {},
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb7b755e",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def riemannian_gradient_hook(grad, mean):\n",
        "    \"\"\"\n",
        "    Hook to project Euclidean gradient onto the tangent space of the unit sphere.\n",
        "    This ensures the gradient update stays on the manifold.\n",
        "    \n",
        "    For the unit sphere, the tangent space projection is:\n",
        "    grad_tangent = grad - (grad · x) * x\n",
        "    \n",
        "    Args:\n",
        "        grad: Euclidean gradient [batch_size, dim]\n",
        "        mean: Points on the unit sphere [batch_size, dim]\n",
        "    \n",
        "    Returns:\n",
        "        Riemannian gradient projected onto tangent space\n",
        "    \"\"\"\n",
        "    # Compute dot product: (grad · x)\n",
        "    dot_product = (grad * mean).sum(dim=-1, keepdim=True)\n",
        "    # Project: grad - (grad · x) * x\n",
        "    riemannian_grad = grad - dot_product * mean\n",
        "    return riemannian_grad\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "        model: VariationalCLIPModel,\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        criterion: torch.nn.Module,\n",
        "        epoch: int,\n",
        "):\n",
        "    model.train()\n",
        "\n",
        "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training Epoch\")\n",
        "    total_loss = 0.0\n",
        "    total_clip_loss = 0.0\n",
        "    total_image_kl_loss = 0.0\n",
        "    total_text_kl_loss = 0.0\n",
        "    total_kl_loss = 0.0\n",
        "    nan_count = 0\n",
        "\n",
        "    for batch_idx, (images, text_tokens, _) in enumerate(progress_bar):\n",
        "        images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
        "        images = images.float()\n",
        "\n",
        "        # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
        "        if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
        "            logger.warning(f\"NaN in input batch {batch_idx}\")\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        image_means, image_concentrations = model.encode_image_tensors(images)\n",
        "        text_means, text_concentrations = model.encode_text_tokens(text_tokens)\n",
        "\n",
        "        # Register hooks to project gradients onto tangent space\n",
        "        # This ensures gradients respect the spherical constraint\n",
        "        if epoch > CONFIG['NUM_EPOCHS_TO_FULL_KL']:\n",
        "            if image_means.requires_grad:\n",
        "                image_means.register_hook(lambda grad: riemannian_gradient_hook(grad, image_means))\n",
        "            if text_means.requires_grad:\n",
        "                text_means.register_hook(lambda grad: riemannian_gradient_hook(grad, text_means))\n",
        "\n",
        "        # Check for NaN in features\n",
        "        nan_image = torch.isnan(image_means).any(\n",
        "        ) or torch.isnan(image_concentrations).any()\n",
        "        nan_text = torch.isnan(text_means).any(\n",
        "        ) or torch.isnan(text_concentrations).any()\n",
        "        if nan_image or nan_text:\n",
        "            logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
        "            nan_count += 1\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        # Debug log for scale parameters before constructing distributions\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"log_concentration_scale\" in name and not torch.isfinite(p).all():\n",
        "                print(f\"Non-finite scale param {name}: {p.data}\")\n",
        "\n",
        "        image_distribution = PowerSpherical(image_means, image_concentrations)\n",
        "        text_distribution = PowerSpherical(text_means, text_concentrations)\n",
        "        loss_dict = criterion(image_distribution, text_distribution, model.get_logits_scale(\n",
        "        ), kl_weight_override=kl_schedule(epoch))\n",
        "        loss = loss_dict['total_loss']\n",
        "\n",
        "        # Check for NaN in loss\n",
        "        if torch.isnan(loss):\n",
        "            logger.warning(f\"NaN loss detected at batch {batch_idx}\")\n",
        "            nan_count += 1\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        # If not NaN, then add to total loss\n",
        "        total_loss += loss.item()\n",
        "        total_clip_loss += loss_dict['clip_loss'].item()\n",
        "        total_image_kl_loss += loss_dict['image_kl_loss'].item()\n",
        "        total_text_kl_loss += loss_dict['text_kl_loss'].item()\n",
        "        total_kl_loss += loss_dict['image_kl_loss'].item() + loss_dict['text_kl_loss'].item()\n",
        "\n",
        "        # Scale loss for gradient accumulation\n",
        "        scaled_loss = loss / CONFIG['GRAD_ACCUMULATION_STEPS']\n",
        "        # Backward pass\n",
        "        scaled_loss.backward()\n",
        "\n",
        "        has_nan_grads = False\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                has_nan_grads = True\n",
        "                logger.warning(f\"NaN gradient in {name}\")\n",
        "                break\n",
        "\n",
        "        if has_nan_grads:\n",
        "            logger.warning(\n",
        "                f\"NaN gradients detected at batch {batch_idx}, skipping update\")\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        # Gradient accumulation and optimization step\n",
        "        if (batch_idx + 1) % CONFIG['GRAD_ACCUMULATION_STEPS'] == 0:\n",
        "            # Clip gradients to prevent explosion\n",
        "            if CONFIG['CLIP_GRADIENTS']:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': total_loss / (batch_idx + 1),\n",
        "                'clip_loss': total_clip_loss / (batch_idx + 1),\n",
        "                'total_kl_loss': total_kl_loss / (batch_idx + 1),\n",
        "                'nan_count': nan_count\n",
        "            })\n",
        "\n",
        "        if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    # Handle remaining gradients if not accumulated evenly\n",
        "    if len(dataloader) % CONFIG['GRAD_ACCUMULATION_STEPS'] != 0:\n",
        "        if CONFIG['CLIP_GRADIENTS']:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    total_loss = total_loss / len(dataloader)\n",
        "    total_clip_loss = total_clip_loss / len(dataloader)\n",
        "    total_image_kl_loss = total_image_kl_loss / len(dataloader)\n",
        "    total_text_kl_loss = total_text_kl_loss / len(dataloader)\n",
        "    total_kl_loss = total_kl_loss / len(dataloader)\n",
        "    return total_loss, total_clip_loss, total_image_kl_loss, total_text_kl_loss, total_kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92349af6",
      "metadata": {},
      "source": [
        "# Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90e1aa39",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def validate(\n",
        "    model: VariationalCLIPModel,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    epoch: int\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    progress_bar = tqdm.tqdm(dataloader, desc=\"Evaluating\")\n",
        "    total_loss = 0.0\n",
        "    total_clip_loss = 0.0\n",
        "    total_image_kl_loss = 0.0\n",
        "    total_text_kl_loss = 0.0\n",
        "    total_kl_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, text_tokens, _) in enumerate(progress_bar):\n",
        "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
        "            images = images.float()\n",
        "\n",
        "            # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
        "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
        "                logger.warning(f\"NaN in input batch {batch_idx}\")\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            image_means, image_concentrations = model.encode_image_tensors(\n",
        "                images)\n",
        "            text_means, text_concentrations = model.encode_text_tokens(\n",
        "                text_tokens)\n",
        "\n",
        "            # Check for NaN in features\n",
        "            nan_image = torch.isnan(image_means).any() or torch.isnan(\n",
        "                image_concentrations).any()\n",
        "            nan_text = torch.isnan(text_means).any() or torch.isnan(\n",
        "                text_concentrations).any()\n",
        "            if nan_image or nan_text:\n",
        "                logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            # Debug log for scale parameters before constructing distributions\n",
        "            for name, p in model.named_parameters():\n",
        "                if \"log_concentration_scale\" in name and not torch.isfinite(p).all():\n",
        "                    print(f\"Non-finite scale param {name}: {p.data}\")\n",
        "\n",
        "            image_distribution = PowerSpherical(image_means,\n",
        "                                                image_concentrations)\n",
        "            text_distribution = PowerSpherical(text_means, text_concentrations)\n",
        "            loss_dict = criterion(image_distribution, text_distribution, model.get_logits_scale(\n",
        "            ), kl_weight_override=kl_schedule(epoch))\n",
        "            loss = loss_dict['total_loss']\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                logger.warning(\"NaN in validation loss, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_clip_loss += loss_dict['clip_loss'].item()\n",
        "            total_image_kl_loss += loss_dict['image_kl_loss'].item()\n",
        "            total_text_kl_loss += loss_dict['text_kl_loss'].item()\n",
        "            total_kl_loss += loss_dict['image_kl_loss'].item() + loss_dict['text_kl_loss'].item()\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': total_loss / (batch_idx + 1),\n",
        "                'clip_loss':  total_clip_loss / (batch_idx + 1),\n",
        "                'total_kl_loss': total_kl_loss / (batch_idx + 1),\n",
        "            })\n",
        "\n",
        "            if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "    total_loss = total_loss / len(dataloader)\n",
        "    total_clip_loss = total_clip_loss / len(dataloader)\n",
        "    total_image_kl_loss = total_image_kl_loss / len(dataloader)\n",
        "    total_text_kl_loss = total_text_kl_loss / len(dataloader)\n",
        "    total_kl_loss = total_kl_loss / len(dataloader)\n",
        "    return total_loss, total_clip_loss, total_image_kl_loss, total_text_kl_loss, total_kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b830231e",
      "metadata": {},
      "source": [
        "# Full Train Eval Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38f854da",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(\n",
        "    filename: str,\n",
        "    model: VariationalCLIPModel,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    train_losses: list[float],\n",
        "    val_losses: list[float],\n",
        "):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'config': CONFIG,\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    logger.info(f\"Checkpoint saved: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbdfd36b",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    num_epochs: int,\n",
        "    model: VariationalCLIPModel,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
        "    criterion: torch.nn.Module\n",
        "):\n",
        "    \"\"\"Train model for specified epochs.\"\"\"\n",
        "    logger.info(f\"Starting training on {DEVICE} for {num_epochs} epochs...\")\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        logger.info(f\"KL Weight: {kl_schedule(epoch):.6f}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_clip, train_img_kl, train_text_kl, train_kl = train_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            epoch\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_clip, val_img_kl, val_text_kl, val_kl = validate(\n",
        "            model,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            epoch\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "        logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "        # Skip if losses are NaN\n",
        "        if torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss)):\n",
        "            logger.error(\"NaN loss detected! Stopping training.\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "        logger.info(\n",
        "            f\"Learning Rate adjusted to: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(\n",
        "                f\"best_VCLIP_model_on_{DATASET}.pt\",\n",
        "                model,\n",
        "                optimizer,\n",
        "                train_losses,\n",
        "                val_losses\n",
        "            )\n",
        "            logger.info(f\"✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
        "\n",
        "        if CONFIG['USE_WANDB']:\n",
        "            wandb.log({\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'train_clip_loss': train_clip,\n",
        "                'val_clip_loss': val_clip,\n",
        "                'train_kl_loss': train_kl,\n",
        "                'val_kl_loss': val_kl,\n",
        "                'kl_weight': kl_schedule(epoch),\n",
        "                'learning_rate': scheduler.get_last_lr()[0],\n",
        "            })\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a52eec8",
      "metadata": {},
      "source": [
        "# WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1c0d65",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Use wandb? Resume Training?\n",
        "PROJECT_NAME = CONFIG['WANDB_PROJECT_NAME']\n",
        "USE_WANDB = CONFIG['USE_WANDB']\n",
        "RESUME_LOGGING = CONFIG['WANDB_PREVIOUS_RUN_ID'] is not None\n",
        "run_name = CONFIG['WANDB_RUN_NAME']\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "    if RESUME_LOGGING:\n",
        "        run_id = CONFIG['WANDB_PREVIOUS_RUN_ID']\n",
        "        run = wandb.init(\n",
        "            settings=wandb.Settings(symlink=False),\n",
        "            id=run_id,\n",
        "            resume=\"must\",\n",
        "            project=PROJECT_NAME,\n",
        "            entity=\"multimodal_2025\",\n",
        "        )\n",
        "    else:\n",
        "        run = wandb.init(\n",
        "            name=run_name,\n",
        "            reinit=True,\n",
        "            project=PROJECT_NAME,\n",
        "            config=CONFIG,\n",
        "            entity=\"multimodal_2025\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72b319d4",
      "metadata": {},
      "source": [
        "# Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ff5222",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e2ef1a5",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_ENABLE_MPS_FALLBACK=1\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "train_losses, val_losses = train(\n",
        "    CONFIG[\"NUM_EPOCHS\"],\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    loss  # type: ignore\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61d88063",
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# grab the current time\n",
        "from datetime import datetime\n",
        "timestamp = datetime.timestamp(datetime.now())\n",
        "\n",
        "# Save best model to /mnt/content/Models\n",
        "os.makedirs('/mnt/content/Models', exist_ok=True)\n",
        "torch.save(model.state_dict(), f\"/mnt/content/Models/best_vclip_model_at_{timestamp}.pth\")\n",
        "logger.info(f\"/mnt/content/Models/best_vclip_model_at_{timestamp}.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
