{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initialization and Config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "IS_COLAB = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if IS_COLAB:   \n",
        "    import os\n",
        "\n",
        "    github_token = \"\"\n",
        "    github_username = \"\" # Replace with your GitHub username\n",
        "    repository_url = f\"https://{github_username}:{github_token}@github.com/neskech/Multimodal-2025.git\"\n",
        "\n",
        "    !git clone {repository_url}\n",
        "    \n",
        "\n",
        "    %cd Multimodal-2025\n",
        "    !git submodule update --init --recursive\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reqs = \"\"\"\n",
        "torch>=1.9.0\n",
        "torchvision>=0.10.0\n",
        "numpy>=1.21.0\n",
        "scipy>=1.7.0\n",
        "scikit-learn>=1.0.0\n",
        "matplotlib>=3.4.0\n",
        "seaborn>=0.11.0\n",
        "Pillow>=8.3.0\n",
        "clip @ git+https://github.com/openai/CLIP.git@dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
        "power-spherical @ git+https://github.com/nicola-decao/power_spherical.git@3d4619a9d6c01bc9b427533d386271a233e304cd\n",
        "dotenv\n",
        "\"\"\"\n",
        "with open(\"/root/Multimodal-2025/requirements.txt\", \"w\") as f:\n",
        "    f.write(reqs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%uv pip install -r requirements.txt\n",
        "%uv pip install wandb\n",
        "%uv pip install webdataset\n",
        "%uv pip install datasets\n",
        "!cd Multimodal-2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if not IS_COLAB: # colab does not seem to support these\n",
        "    %load_ext autoreload\n",
        "    %autoreload 2\n",
        "    %reload_ext autoreload\n",
        "\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import logging\n",
        "import wandb\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "from typing import Literal, Union\n",
        "\n",
        "sys.path.append(\"..\")\n",
        "sys.path.append(\"Multimodal-2025\")\n",
        "from sklearn.decomposition import PCA\n",
        "from Datasets.coco import CocoDataset\n",
        "from Datasets.cc12m import CC12mDataset\n",
        "from Datasets.cood import CoodDataset\n",
        "from Datasets.laion import LaionDataset\n",
        "from Models.variationalClip import VariationalCLIPModel\n",
        "from losses.vclipLoss import VClipLoss, power_spherical_mean\n",
        "from power_spherical import PowerSpherical\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Type aliases for compatibility with older Python versions\n",
        "type Dataset = Literal['COCO', 'COOD', 'CC12M', 'LAION']\n",
        "DATASET: Dataset = 'COCO'\n",
        "\n",
        "DEVICE = None\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'\n",
        "elif torch.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "else:\n",
        "    DEVICE = 'cpu'\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get WANDB Key (Use a .env file to store the key)\n",
        "load_dotenv()\n",
        "WANDB_API_KEY = os.environ.get('WANDB_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'NUM_EPOCHS': 20,\n",
        "    'BATCH_SIZE': 256,\n",
        "    'LEARNING_RATE': 1e-3,\n",
        "    'WEIGHT_DECAY': 1e-8,\n",
        "\n",
        "    # Scheduler parameters\n",
        "    'WARMUP_EPOCHS': 2,\n",
        "    'DECAY_EPOCHS': 10,\n",
        "\n",
        "    # To avoid gradient explosion. Set to 1 to disable\n",
        "    'GRAD_ACCUMULATION_STEPS': 1,\n",
        "    # Clip gradients to avoid explosion\n",
        "    'CLIP_GRADIENTS': False,\n",
        "    'EMPTY_CACHE_AFTER_BATCH': False,\n",
        "\n",
        "    # False to disable preloaded weights\n",
        "    'LOAD_PRETRAINED_WEIGHTS': False,\n",
        "\n",
        "    'KL_WEIGHT': 1,\n",
        "    'NUM_EPOCHS_TO_FULL_KL': 1,\n",
        "\n",
        "    'DATA_DIR': '/mnt/content/Data',\n",
        "    'TRAIN_RATIO': 0.9,\n",
        "    'TOTAL_DATAPOINTS': 10_000,\n",
        "\n",
        "    'USE_WANDB': True,\n",
        "    'WANDB_RUN_NAME': f'VCLIP_Train_On_{DATASET}_{50_000}',\n",
        "    'WANDB_PREVIOUS_RUN_ID': None,  # set to None if not resuming\n",
        "    'WANDB_PROJECT_NAME': 'multimodal_2025',\n",
        "\n",
        "    'FREEZE_BACKBONE': False,\n",
        "    'USE_GRADIENT_CONSTRAINT': False\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "num_train = int(CONFIG['TOTAL_DATAPOINTS'] * CONFIG['TRAIN_RATIO'])\n",
        "num_val = int(CONFIG['TOTAL_DATAPOINTS'] * (1.0 - CONFIG['TRAIN_RATIO']))\n",
        "\n",
        "if DATASET == 'COCO':\n",
        "    train = CocoDataset(\n",
        "        data_dir=\"/mnt/content/Data\",\n",
        "        split='train2017',\n",
        "        tokenize=True,\n",
        "        max_samples=num_train\n",
        "    )\n",
        "    val = CocoDataset(\n",
        "        data_dir=\"/mnt/content/Data\",\n",
        "        split='val2017',\n",
        "        tokenize=True,\n",
        "        max_samples=num_val\n",
        "    )\n",
        "    collate_fn = CocoDataset.collate_function\n",
        "elif DATASET == 'COOD':\n",
        "    CoodDataset.download(data_dir=CONFIG['DATA_DIR'])\n",
        "    all_data = CoodDataset(\n",
        "        data_dir=CONFIG['DATA_DIR'],\n",
        "        tokenize=True,\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
        "    )\n",
        "    train = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(0, num_train)\n",
        "    )\n",
        "    val = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
        "    )\n",
        "    collate_fn = CoodDataset.collate_function\n",
        "elif DATASET == 'LAION':\n",
        "    LaionDataset.download(\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
        "    all_data = LaionDataset(\n",
        "        data_dir=CONFIG['DATA_DIR'],\n",
        "        tokenize=True,\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
        "    )\n",
        "    train = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(0, num_train)\n",
        "    )\n",
        "    val = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
        "    )\n",
        "    collate_fn = LaionDataset.collate_function\n",
        "elif DATASET == 'CC12M':\n",
        "    CC12mDataset.download(\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
        "    all_data = CC12mDataset(\n",
        "        data_dir=CONFIG['DATA_DIR'],\n",
        "        tokenize=True,\n",
        "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
        "    )\n",
        "    train = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(0, num_train)\n",
        "    )\n",
        "    val = torch.utils.data.Subset(\n",
        "        all_data,\n",
        "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
        "    )\n",
        "    collate_fn = CC12mDataset.collate_function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train,\n",
        "    CONFIG['BATCH_SIZE'],\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    pin_memory=DEVICE == 'cuda',\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val,\n",
        "    CONFIG['BATCH_SIZE'],\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=DEVICE == 'cuda',\n",
        "    collate_fn=collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "logger.info(\n",
        "    f\"Training on {len(train)} samples, validating on {len(val)} samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model = VariationalCLIPModel(\n",
        "    'Spherical', DEVICE, use_pretrained=CONFIG['LOAD_PRETRAINED_WEIGHTS'])\n",
        "model = model.float().to(DEVICE).freeze_backbone(CONFIG['FREEZE_BACKBONE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "loss = VClipLoss(kl_weight=CONFIG['KL_WEIGHT'],\n",
        "                 use_mean_only=False, label_smoothing=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CONFIG['LEARNING_RATE'],\n",
        "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
        ")\n",
        "\n",
        "# Warmup scheduler: Linear increase from 0 to target_lr\n",
        "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "    optimizer, start_factor=0.01, end_factor=1.0, total_iters=CONFIG['WARMUP_EPOCHS'])\n",
        "\n",
        "# Decay scheduler: Cosine annealing after warmup\n",
        "decay_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=CONFIG['DECAY_EPOCHS'])\n",
        "\n",
        "# Combine them using SequentialLR\n",
        "# The schedulers will be applied sequentially\n",
        "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[\n",
        "                                                  warmup_scheduler, decay_scheduler], milestones=[CONFIG['WARMUP_EPOCHS']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def kl_schedule(epoch: int) -> float:\n",
        "    \"\"\"Returns the KL weight for the given epoch based on a linear schedule.\"\"\"\n",
        "    epoch = epoch + 1  # Epochs are 0-indexed\n",
        "    if epoch < CONFIG['NUM_EPOCHS_TO_FULL_KL']:\n",
        "        return 0\n",
        "    if epoch >= 2 * CONFIG['NUM_EPOCHS_TO_FULL_KL']:\n",
        "        return CONFIG['KL_WEIGHT']\n",
        "    else:\n",
        "        return CONFIG['KL_WEIGHT'] * (epoch / (2 * CONFIG['NUM_EPOCHS_TO_FULL_KL']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def riemannian_gradient_hook(grad, mean):\n",
        "    \"\"\"\n",
        "    Hook to project Euclidean gradient onto the tangent space of the unit sphere.\n",
        "    This ensures the gradient update stays on the manifold.\n",
        "\n",
        "    For the unit sphere, the tangent space projection is:\n",
        "    grad_tangent = grad - (grad · x) * x\n",
        "\n",
        "    Args:\n",
        "        grad: Euclidean gradient [batch_size, dim]\n",
        "        mean: Points on the unit sphere [batch_size, dim]\n",
        "\n",
        "    Returns:\n",
        "        Riemannian gradient projected onto tangent space\n",
        "    \"\"\"\n",
        "    # Compute dot product: (grad · x)\n",
        "    dot_product = (grad * mean).sum(dim=-1, keepdim=True)\n",
        "    # Project: grad - (grad · x) * x\n",
        "    riemannian_grad = grad - dot_product * mean\n",
        "    return riemannian_grad\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "        model: VariationalCLIPModel,\n",
        "        dataloader: torch.utils.data.DataLoader,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        criterion: torch.nn.Module,\n",
        "        epoch: int,\n",
        "):\n",
        "    model.train()\n",
        "\n",
        "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training Epoch\")\n",
        "    total_loss = 0.0\n",
        "    total_clip_loss = 0.0\n",
        "    total_image_kl_loss = 0.0\n",
        "    total_text_kl_loss = 0.0\n",
        "    total_kl_loss = 0.0\n",
        "    nan_count = 0\n",
        "\n",
        "    for batch_idx, (images, text_tokens, _) in enumerate(progress_bar):\n",
        "        images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
        "        images = images.float()\n",
        "\n",
        "        # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
        "        if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
        "            logger.warning(f\"NaN in input batch {batch_idx}\")\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        image_means, image_concentrations = model.encode_image_tensors(images)\n",
        "        text_means, text_concentrations = model.encode_text_tokens(text_tokens)\n",
        "\n",
        "        # Register hooks to project gradients onto tangent space\n",
        "        # This ensures gradients respect the spherical constraint\n",
        "        if CONFIG['USE_GRADIENT_CONSTRAINT']:\n",
        "            if image_means.requires_grad:\n",
        "                image_means.register_hook(\n",
        "                    lambda grad: riemannian_gradient_hook(grad, image_means))\n",
        "            if text_means.requires_grad:\n",
        "                text_means.register_hook(\n",
        "                    lambda grad: riemannian_gradient_hook(grad, text_means))\n",
        "\n",
        "        # Check for NaN in features\n",
        "        nan_image = torch.isnan(image_means).any(\n",
        "        ) or torch.isnan(image_concentrations).any()\n",
        "        nan_text = torch.isnan(text_means).any(\n",
        "        ) or torch.isnan(text_concentrations).any()\n",
        "        if nan_image or nan_text:\n",
        "            logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
        "            nan_count += 1\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        # Debug log for scale parameters before constructing distributions\n",
        "        for name, p in model.named_parameters():\n",
        "            if \"log_concentration_scale\" in name and not torch.isfinite(p).all():\n",
        "                print(f\"Non-finite scale param {name}: {p.data}\")\n",
        "\n",
        "        image_distribution = PowerSpherical(image_means, image_concentrations)\n",
        "        text_distribution = PowerSpherical(text_means, text_concentrations)\n",
        "        loss_dict = criterion(image_distribution, text_distribution, model.get_logits_scale(\n",
        "        ), kl_weight_override=kl_schedule(epoch))\n",
        "        loss = loss_dict['total_loss']\n",
        "\n",
        "        # Check for NaN in loss\n",
        "        if torch.isnan(loss):\n",
        "            logger.warning(f\"NaN loss detected at batch {batch_idx}\")\n",
        "            nan_count += 1\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        # If not NaN, then add to total loss\n",
        "        total_loss += loss.item()\n",
        "        total_clip_loss += loss_dict['clip_loss'].item()\n",
        "        total_image_kl_loss += loss_dict['image_kl_loss'].item()\n",
        "        total_text_kl_loss += loss_dict['text_kl_loss'].item()\n",
        "        total_kl_loss += loss_dict['image_kl_loss'].item() + \\\n",
        "            loss_dict['text_kl_loss'].item()\n",
        "\n",
        "        # Scale loss for gradient accumulation\n",
        "        scaled_loss = loss / CONFIG['GRAD_ACCUMULATION_STEPS']\n",
        "        # Backward pass\n",
        "        scaled_loss.backward()\n",
        "\n",
        "        has_nan_grads = False\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                has_nan_grads = True\n",
        "                logger.warning(f\"NaN gradient in {name}\")\n",
        "                break\n",
        "\n",
        "        if has_nan_grads:\n",
        "            logger.warning(\n",
        "                f\"NaN gradients detected at batch {batch_idx}, skipping update\")\n",
        "            optimizer.zero_grad()\n",
        "            continue\n",
        "\n",
        "        # Gradient accumulation and optimization step\n",
        "        if (batch_idx + 1) % CONFIG['GRAD_ACCUMULATION_STEPS'] == 0:\n",
        "            # Clip gradients to prevent explosion\n",
        "            if CONFIG['CLIP_GRADIENTS']:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': total_loss / (batch_idx + 1),\n",
        "                'clip_loss': total_clip_loss / (batch_idx + 1),\n",
        "                'total_kl_loss': total_kl_loss / (batch_idx + 1),\n",
        "                'nan_count': nan_count\n",
        "            })\n",
        "\n",
        "        if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "    # Handle remaining gradients if not accumulated evenly\n",
        "    if len(dataloader) % CONFIG['GRAD_ACCUMULATION_STEPS'] != 0:\n",
        "        if CONFIG['CLIP_GRADIENTS']:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    total_loss = total_loss / len(dataloader)\n",
        "    total_clip_loss = total_clip_loss / len(dataloader)\n",
        "    total_image_kl_loss = total_image_kl_loss / len(dataloader)\n",
        "    total_text_kl_loss = total_text_kl_loss / len(dataloader)\n",
        "    total_kl_loss = total_kl_loss / len(dataloader)\n",
        "    return total_loss, total_clip_loss, total_image_kl_loss, total_text_kl_loss, total_kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Validation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def validate(\n",
        "    model: VariationalCLIPModel,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    epoch: int\n",
        "):\n",
        "    model.eval()\n",
        "\n",
        "    progress_bar = tqdm.tqdm(dataloader, desc=\"Evaluating\")\n",
        "    total_loss = 0.0\n",
        "    total_clip_loss = 0.0\n",
        "    total_image_kl_loss = 0.0\n",
        "    total_text_kl_loss = 0.0\n",
        "    total_kl_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, text_tokens, _) in enumerate(progress_bar):\n",
        "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
        "            images = images.float()\n",
        "\n",
        "            # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
        "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
        "                logger.warning(f\"NaN in input batch {batch_idx}\")\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            image_means, image_concentrations = model.encode_image_tensors(\n",
        "                images)\n",
        "            text_means, text_concentrations = model.encode_text_tokens(\n",
        "                text_tokens)\n",
        "\n",
        "            # Check for NaN in features\n",
        "            nan_image = torch.isnan(image_means).any() or torch.isnan(\n",
        "                image_concentrations).any()\n",
        "            nan_text = torch.isnan(text_means).any() or torch.isnan(\n",
        "                text_concentrations).any()\n",
        "            if nan_image or nan_text:\n",
        "                logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
        "                optimizer.zero_grad()\n",
        "                continue\n",
        "\n",
        "            image_distribution = PowerSpherical(image_means,\n",
        "                                                image_concentrations)\n",
        "            text_distribution = PowerSpherical(text_means, text_concentrations)\n",
        "            loss_dict = criterion(image_distribution, text_distribution, model.get_logits_scale(\n",
        "            ), kl_weight_override=kl_schedule(epoch))\n",
        "            loss = loss_dict['total_loss']\n",
        "\n",
        "            # Check for NaN loss\n",
        "            if torch.isnan(loss):\n",
        "                logger.warning(\"NaN in validation loss, skipping batch\")\n",
        "                continue\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_clip_loss += loss_dict['clip_loss'].item()\n",
        "            total_image_kl_loss += loss_dict['image_kl_loss'].item()\n",
        "            total_text_kl_loss += loss_dict['text_kl_loss'].item()\n",
        "            total_kl_loss += total_image_kl_loss + total_text_kl_loss\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': total_loss / (batch_idx + 1),\n",
        "                'clip_loss':  total_clip_loss / (batch_idx + 1),\n",
        "                'total_kl_loss': total_kl_loss / (batch_idx + 1),\n",
        "            })\n",
        "\n",
        "            if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "\n",
        "    total_loss = total_loss / len(dataloader)\n",
        "    total_clip_loss = total_clip_loss / len(dataloader)\n",
        "    total_image_kl_loss = total_image_kl_loss / len(dataloader)\n",
        "    total_text_kl_loss = total_text_kl_loss / len(dataloader)\n",
        "    total_kl_loss = total_kl_loss / len(dataloader)\n",
        "    return total_loss, total_clip_loss, total_image_kl_loss, total_text_kl_loss, total_kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full Train Eval Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(\n",
        "    filename: str,\n",
        "    model: VariationalCLIPModel,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    train_losses: list[float],\n",
        "    val_losses: list[float],\n",
        "):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'config': CONFIG,\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    logger.info(f\"Checkpoint saved: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def generate_pca_visualization(\n",
        "    model: VariationalCLIPModel,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    epoch: int,\n",
        "    max_samples: int = 500\n",
        "):\n",
        "    \"\"\"Generate PCA visualization of validation embeddings and log to wandb.\"\"\"\n",
        "    model.eval()\n",
        "    image_embeddings = []\n",
        "    text_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (images, text_tokens, cap) in enumerate(val_loader):\n",
        "            if len(image_embeddings) >= max_samples:\n",
        "                break\n",
        "\n",
        "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
        "            images = images.float()\n",
        "\n",
        "            # Skip samples with NaN\n",
        "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
        "                continue\n",
        "\n",
        "            image_means, image_concentrations = model.encode_image_tensors(\n",
        "                images)\n",
        "            text_means, text_concentrations = model.encode_text_tokens(\n",
        "                text_tokens)\n",
        "            image_distribution = PowerSpherical(\n",
        "                image_means, image_concentrations)\n",
        "            text_distribution = PowerSpherical(text_means, text_concentrations)\n",
        "            image_features = power_spherical_mean(\n",
        "                image_distribution).detach()  # [batch, dim]\n",
        "            text_features = power_spherical_mean(\n",
        "                text_distribution).detach()    # [batch, dim]\n",
        "\n",
        "            # Skip if NaN in features\n",
        "            if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
        "                continue\n",
        "\n",
        "            # Iterate over each sample in the batch\n",
        "            for i in range(image_features.shape[0]):\n",
        "                if len(image_embeddings) >= max_samples:\n",
        "                    break\n",
        "                image_embeddings.append(image_features[i].cpu().numpy())\n",
        "                text_embeddings.append(text_features[i].cpu().numpy())\n",
        "\n",
        "    if len(image_embeddings) == 0:\n",
        "        logger.warning(\"No valid embeddings for PCA visualization\")\n",
        "        return\n",
        "\n",
        "    # Normalize embeddings\n",
        "    embeddings = np.array([e / (np.linalg.norm(e) + 1e-8) for e in image_embeddings] +\n",
        "                          [e / (np.linalg.norm(e) + 1e-8) for e in text_embeddings])\n",
        "\n",
        "    # Apply PCA\n",
        "    reducer = PCA(n_components=2)\n",
        "    coords_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "    # Create visualization\n",
        "    fig = plt.figure(figsize=(10, 8))\n",
        "    types = [\"image\"] * len(image_embeddings) + [\"text\"] * len(text_embeddings)\n",
        "\n",
        "    # Scatter embeddings\n",
        "    for i, (emb, typ) in enumerate(zip(coords_2d, types)):\n",
        "        color = \"r\" if typ == \"image\" else \"b\"\n",
        "        plt.scatter(emb[0], emb[1], color=color, s=40, alpha=0.8)\n",
        "\n",
        "    # Draw lines between image-text pairs\n",
        "    n = len(image_embeddings)\n",
        "    for i in range(n):\n",
        "        img_coord = coords_2d[i]\n",
        "        text_coord = coords_2d[i + n]\n",
        "        plt.plot(\n",
        "            [img_coord[0], text_coord[0]],\n",
        "            [img_coord[1], text_coord[1]],\n",
        "            color=\"gray\",\n",
        "            alpha=0.3,\n",
        "            linewidth=0.7,\n",
        "        )\n",
        "\n",
        "    plt.title(f\"PCA of Val Embeddings - Epoch {epoch + 1}\")\n",
        "    plt.xlabel(\"PC1\")\n",
        "    plt.ylabel(\"PC2\")\n",
        "    plt.legend(handles=[\n",
        "        plt.Line2D([0], [0], marker='o', color='w',\n",
        "                   markerfacecolor='r', markersize=10, label='Image'),\n",
        "        plt.Line2D([0], [0], marker='o', color='w',\n",
        "                   markerfacecolor='b', markersize=10, label='Text')\n",
        "    ])\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Display in cell output\n",
        "    display(fig)\n",
        "\n",
        "    # Log to wandb\n",
        "    if CONFIG['USE_WANDB']:\n",
        "        wandb.log({f\"pca_val_embeddings_epoch_{epoch + 1}\": wandb.Image(fig)})\n",
        "\n",
        "    plt.close(fig)\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def train(\n",
        "    num_epochs: int,\n",
        "    model: VariationalCLIPModel,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
        "    criterion: torch.nn.Module\n",
        "):\n",
        "    \"\"\"Train model for specified epochs.\"\"\"\n",
        "    logger.info(f\"Starting training on {DEVICE} for {num_epochs} epochs...\")\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        logger.info(f\"KL Weight: {kl_schedule(epoch):.6f}\")\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_clip, train_img_kl, train_text_kl, train_kl = train_epoch(\n",
        "            model,\n",
        "            train_loader,\n",
        "            optimizer,\n",
        "            criterion,\n",
        "            epoch\n",
        "        )\n",
        "        train_losses.append(train_loss)\n",
        "        logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_clip, val_img_kl, val_text_kl, val_kl = validate(\n",
        "            model,\n",
        "            val_loader,\n",
        "            criterion,\n",
        "            epoch\n",
        "        )\n",
        "        val_losses.append(val_loss)\n",
        "        logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "        # Skip if losses are NaN\n",
        "        if torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss)):\n",
        "            logger.error(\"NaN loss detected! Stopping training.\")\n",
        "            break\n",
        "\n",
        "        scheduler.step()\n",
        "        logger.info(\n",
        "            f\"Learning Rate adjusted to: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_checkpoint(\n",
        "                f\"best_VCLIP_model_on_{DATASET}.pt\",\n",
        "                model,\n",
        "                optimizer,\n",
        "                train_losses,\n",
        "                val_losses\n",
        "            )\n",
        "            logger.info(f\"✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
        "\n",
        "        if CONFIG['USE_WANDB']:\n",
        "            wandb.log({\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "                'train_clip_loss': train_clip,\n",
        "                'val_clip_loss': val_clip,\n",
        "                'train_kl_loss': train_kl,\n",
        "                'val_kl_loss': val_kl,\n",
        "                'kl_weight': kl_schedule(epoch),\n",
        "                'learning_rate': scheduler.get_last_lr()[0],\n",
        "            })\n",
        "\n",
        "        # Generate PCA visualization every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "            logger.info(\n",
        "                f\"Generating PCA visualization for epoch {epoch + 1}...\")\n",
        "            generate_pca_visualization(model, val_loader, epoch)\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "WANDB_API_KEY = \"b46761675741dfd30bb73895f496f2ea707e6572\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Use wandb? Resume Training?\n",
        "PROJECT_NAME = CONFIG['WANDB_PROJECT_NAME']\n",
        "USE_WANDB = CONFIG['USE_WANDB']\n",
        "RESUME_LOGGING = CONFIG['WANDB_PREVIOUS_RUN_ID'] is not None\n",
        "run_name = CONFIG['WANDB_RUN_NAME']\n",
        "\n",
        "if USE_WANDB:\n",
        "    wandb.login(key=WANDB_API_KEY)\n",
        "\n",
        "    if RESUME_LOGGING:\n",
        "        run_id = CONFIG['WANDB_PREVIOUS_RUN_ID']\n",
        "        run = wandb.init(\n",
        "            settings=wandb.Settings(symlink=False),\n",
        "            id=run_id,\n",
        "            resume=\"must\",\n",
        "            project=PROJECT_NAME,\n",
        "            entity=\"multimodal_2025\",\n",
        "        )\n",
        "    else:\n",
        "        run = wandb.init(\n",
        "            name=run_name,\n",
        "            reinit=True,\n",
        "            project=PROJECT_NAME,\n",
        "            config=CONFIG,\n",
        "            entity=\"multimodal_2025\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ecfe37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ablation configurations\n",
        "ABLATION_CONFIGS = [\n",
        "    # (KL_WEIGHT, LOAD_PRETRAINED + FREEZE_BACKBONE, USE_GRADIENT_CONSTRAINT)\n",
        "    # KL Weight variations\n",
        "    {'KL_WEIGHT': 10, 'LOAD_PRETRAINED_WEIGHTS': True,\n",
        "        'FREEZE_BACKBONE': True, 'USE_GRADIENT_CONSTRAINT': False},\n",
        "    {'KL_WEIGHT': 1, 'LOAD_PRETRAINED_WEIGHTS': True,\n",
        "        'FREEZE_BACKBONE': True, 'USE_GRADIENT_CONSTRAINT': False},\n",
        "    {'KL_WEIGHT': 0.1, 'LOAD_PRETRAINED_WEIGHTS': True,\n",
        "        'FREEZE_BACKBONE': True, 'USE_GRADIENT_CONSTRAINT': False},\n",
        "\n",
        "    # Pretrained vs Not Pretrained (with KL=1)\n",
        "    {'KL_WEIGHT': 1, 'LOAD_PRETRAINED_WEIGHTS': False,\n",
        "        'FREEZE_BACKBONE': False, 'USE_GRADIENT_CONSTRAINT': False},\n",
        "\n",
        "    # Gradient constraint variations (with KL=1)\n",
        "    {'KL_WEIGHT': 1, 'LOAD_PRETRAINED_WEIGHTS': True,\n",
        "        'FREEZE_BACKBONE': True, 'USE_GRADIENT_CONSTRAINT': True},\n",
        "    {'KL_WEIGHT': 1, 'LOAD_PRETRAINED_WEIGHTS': False,\n",
        "        'FREEZE_BACKBONE': False, 'USE_GRADIENT_CONSTRAINT': True},\n",
        "\n",
        "    # Full combinations with gradient constraint\n",
        "    {'KL_WEIGHT': 10, 'LOAD_PRETRAINED_WEIGHTS': True,\n",
        "        'FREEZE_BACKBONE': True, 'USE_GRADIENT_CONSTRAINT': True},\n",
        "    {'KL_WEIGHT': 10, 'LOAD_PRETRAINED_WEIGHTS': False,\n",
        "        'FREEZE_BACKBONE': False, 'USE_GRADIENT_CONSTRAINT': False},\n",
        "    {'KL_WEIGHT': 0.1, 'LOAD_PRETRAINED_WEIGHTS': True,\n",
        "        'FREEZE_BACKBONE': True, 'USE_GRADIENT_CONSTRAINT': True},\n",
        "    {'KL_WEIGHT': 0.1, 'LOAD_PRETRAINED_WEIGHTS': False,\n",
        "        'FREEZE_BACKBONE': False, 'USE_GRADIENT_CONSTRAINT': False},\n",
        "]\n",
        "\n",
        "\n",
        "def run_ablation(ablation_config: dict):\n",
        "    \"\"\"Run a single ablation experiment.\"\"\"\n",
        "    global model, optimizer, scheduler, loss\n",
        "\n",
        "    # Update CONFIG with ablation settings\n",
        "    for key, value in ablation_config.items():\n",
        "        CONFIG[key] = value\n",
        "\n",
        "    # Create descriptive run name\n",
        "    kl_w = ablation_config['KL_WEIGHT']\n",
        "    pretrained = \"pretrained\" if ablation_config['LOAD_PRETRAINED_WEIGHTS'] else \"scratch\"\n",
        "    frozen = \"frozen\" if ablation_config['FREEZE_BACKBONE'] else \"unfrozen\"\n",
        "    grad_const = \"gradConst\" if ablation_config['USE_GRADIENT_CONSTRAINT'] else \"noGradConst\"\n",
        "    run_name = f\"VCLIP_{DATASET}_KL{kl_w}_{pretrained}_{frozen}_{grad_const}\"\n",
        "\n",
        "    logger.info(f\"\\n{'='*60}\")\n",
        "    logger.info(f\"Starting ablation: {run_name}\")\n",
        "    logger.info(f\"Config: {ablation_config}\")\n",
        "    logger.info(f\"{'='*60}\\n\")\n",
        "\n",
        "    # Reinitialize model\n",
        "    model = VariationalCLIPModel(\n",
        "        'Spherical', DEVICE, use_pretrained=CONFIG['LOAD_PRETRAINED_WEIGHTS'])\n",
        "    model = model.float().to(DEVICE).freeze_backbone(CONFIG['FREEZE_BACKBONE'])\n",
        "\n",
        "    # Reinitialize loss\n",
        "    loss = VClipLoss(kl_weight=CONFIG['KL_WEIGHT'],\n",
        "                     use_mean_only=False, label_smoothing=0.0)\n",
        "\n",
        "    # Reinitialize optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CONFIG['LEARNING_RATE'],\n",
        "        weight_decay=CONFIG['WEIGHT_DECAY']\n",
        "    )\n",
        "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer, start_factor=0.01, end_factor=1.0, total_iters=CONFIG['WARMUP_EPOCHS']\n",
        "    )\n",
        "    decay_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "        optimizer, T_max=CONFIG['DECAY_EPOCHS']\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.SequentialLR(\n",
        "        optimizer, schedulers=[warmup_scheduler, decay_scheduler], milestones=[\n",
        "            CONFIG['WARMUP_EPOCHS']]\n",
        "    )\n",
        "\n",
        "    # Initialize wandb for this run\n",
        "    if CONFIG['USE_WANDB']:\n",
        "        # Finish any existing run\n",
        "        if wandb.run is not None:\n",
        "            wandb.finish()\n",
        "\n",
        "        wandb.init(\n",
        "            name=run_name,\n",
        "            reinit=True,\n",
        "            project=CONFIG['WANDB_PROJECT_NAME'],\n",
        "            config={**CONFIG, **ablation_config},\n",
        "            entity=\"multimodal_2025\",\n",
        "            tags=[\"ablation\", f\"kl_{kl_w}\", pretrained, frozen, grad_const]\n",
        "        )\n",
        "\n",
        "    # Run training\n",
        "    train_losses, val_losses = train(\n",
        "        CONFIG[\"NUM_EPOCHS\"],\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        loss\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    from datetime import datetime\n",
        "    timestamp = datetime.timestamp(datetime.now())\n",
        "    os.makedirs('/mnt/content/Models', exist_ok=True)\n",
        "    model_path = f\"/mnt/content/Models/{run_name}_{timestamp}.pth\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    logger.info(f\"Saved model to {model_path}\")\n",
        "\n",
        "    # Log final metrics\n",
        "    if CONFIG['USE_WANDB']:\n",
        "        wandb.log({\n",
        "            'final_train_loss': train_losses[-1] if train_losses else None,\n",
        "            'final_val_loss': val_losses[-1] if val_losses else None,\n",
        "        })\n",
        "        wandb.finish()\n",
        "\n",
        "    # Clear memory\n",
        "    del model, optimizer, scheduler\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "    gc.collect()\n",
        "\n",
        "    return {'run_name': run_name, 'train_losses': train_losses, 'val_losses': val_losses}\n",
        "\n",
        "\n",
        "def run_all_ablations():\n",
        "    \"\"\"Run all ablation experiments.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for i, config in enumerate(ABLATION_CONFIGS):\n",
        "        logger.info(f\"\\n\\n{'#'*60}\")\n",
        "        logger.info(f\"ABLATION {i+1}/{len(ABLATION_CONFIGS)}\")\n",
        "        logger.info(f\"{'#'*60}\\n\")\n",
        "\n",
        "        try:\n",
        "            result = run_ablation(config)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Ablation failed: {e}\")\n",
        "            results.append({'config': config, 'error': str(e)})\n",
        "\n",
        "            # Try to clean up\n",
        "            if wandb.run is not None:\n",
        "                wandb.finish()\n",
        "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "            gc.collect()\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Print ablation summary\n",
        "print(\"Ablation experiments to run:\")\n",
        "for i, cfg in enumerate(ABLATION_CONFIGS):\n",
        "    print(f\"  {i+1}. KL={cfg['KL_WEIGHT']}, Pretrained={cfg['LOAD_PRETRAINED_WEIGHTS']}, \"\n",
        "          f\"Frozen={cfg['FREEZE_BACKBONE']}, GradConstraint={cfg['USE_GRADIENT_CONSTRAINT']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53339c1f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all ablations\n",
        "# NOTE: This will take a long time! Each experiment runs for NUM_EPOCHS epochs.\n",
        "# Estimated time: ~10 experiments x NUM_EPOCHS epochs each\n",
        "\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "ablation_results = run_all_ablations()\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ABLATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "for result in ablation_results:\n",
        "    if 'error' in result:\n",
        "        print(\n",
        "            f\"❌ {result.get('config', 'Unknown')}: FAILED - {result['error']}\")\n",
        "    else:\n",
        "        final_val = result['val_losses'][-1] if result['val_losses'] else 'N/A'\n",
        "        print(f\"✓ {result['run_name']}: Final Val Loss = {final_val:.6f}\" if isinstance(\n",
        "            final_val, float) else f\"✓ {result['run_name']}: Final Val Loss = {final_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_ENABLE_MPS_FALLBACK=1\n",
        "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
        "train_losses, val_losses = train(\n",
        "    CONFIG[\"NUM_EPOCHS\"],\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    loss  # type: ignore\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# grab the current time\n",
        "from datetime import datetime\n",
        "timestamp = datetime.timestamp(datetime.now())\n",
        "\n",
        "# Save best model to /mnt/content/Models\n",
        "os.makedirs('/mnt/content/Models', exist_ok=True)\n",
        "torch.save(model.state_dict(),\n",
        "           f\"/mnt/content/Models/best_vclip_model_at_{timestamp}.pth\")\n",
        "logger.info(f\"/mnt/content/Models/best_vclip_model_at_{timestamp}.pth\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
