{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eafb47b",
   "metadata": {},
   "source": [
    "# Initialization and Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee0a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cloob_training'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlaion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LaionDataset\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclipModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLIPModel\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloobModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CLOOBModel\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvClipModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VariationalCLIPModel\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mModels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01malignClipModel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AlignCLIPModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/School/Senior/temp/Multimodal-2025/Notebooks/../Models/cloobModel.py:19\u001b[39m\n\u001b[32m     16\u001b[39m sys.path.append(os.path.join(os.path.dirname(\u001b[34m__file__\u001b[39m), \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m                              \u001b[33m'\u001b[39m\u001b[33mcloob-training\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mDatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreProcess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clip_preprocessor\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcloob_training\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_pt, pretrained\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Global constant for model configuration\u001b[39;00m\n\u001b[32m     23\u001b[39m MODEL_NAME = \u001b[33m\"\u001b[39m\u001b[33mcloob_laion_400m_vit_b_16_32_epochs\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cloob_training'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from Datasets.coco import CocoDataset\n",
    "from Datasets.cc12m import CC12mDataset\n",
    "from Datasets.cood import CoodDataset\n",
    "from Datasets.laion import LaionDataset\n",
    "from Models.clipModel import CLIPModel\n",
    "from Models.cloobModel import CLOOBModel\n",
    "from Models.vClipModel import VariationalCLIPModel\n",
    "from Models.alignClipModel import AlignCLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd23854",
   "metadata": {},
   "outputs": [],
   "source": [
    "type Model = Literal['CLIP', 'CLOOB', 'ALIGN', 'VCLIP']\n",
    "type ModelClass = CLIPModel | CLOOBModel | AlignCLIPModel | VariationalCLIPModel\n",
    "MODEL: Model = 'CLIP'\n",
    "\n",
    "type Dataset = Literal['COCO', 'COOD', 'CC12M', 'LAION']\n",
    "DATASET: Dataset = 'COCO'\n",
    "\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get WANDB Key\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.environ.get('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef7f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'BATCH_SIZE': 50,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'WEIGHT_DECAY': 1e-1,\n",
    "\n",
    "    'STEP_LR_STEP_SIZE': 5,\n",
    "    'STEP_LR_GAMMA': 0.5,\n",
    "\n",
    "    # To avoid gradient explosion. Set to 1 to disable\n",
    "    'GRAD_ACCUMULATION_STEPS': 1,\n",
    "    'CLIP_GRADIENTS': False,\n",
    "    'EMPTY_CACHE_AFTER_BATCH': False,\n",
    "\n",
    "    'TRAIN_RATIO': 0.8,\n",
    "    'DATA_DIR': '../Data',\n",
    "    'TOTAL_DATAPOINTS': 10_000,\n",
    "\n",
    "    'USE_LORA': False,\n",
    "\n",
    "    'USE_WANDB': True,\n",
    "    'WANDB_RUN_NAME': 'experiment_1',\n",
    "    'WANDB_PREVIOUS_RUN_ID': None, # set to None if not resuming\n",
    "    'WANDB_PROJECT_NAME': 'multimodal_2025',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf6745",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(CONFIG['TOTAL_DATAPOINTS'] * CONFIG['TRAIN_RATIO'])\n",
    "num_val = int(CONFIG['TOTAL_DATAPOINTS'] * (1.0 - CONFIG['TRAIN_RATIO']))\n",
    "\n",
    "if DATASET == 'COCO':\n",
    "    train = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='train2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_train\n",
    "    )\n",
    "    val = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='val2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_val\n",
    "    )   \n",
    "elif DATASET == 'COOD':\n",
    "    all_data = CoodDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "elif DATASET == 'LAION':\n",
    "    all_data = LaionDataset(\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "\n",
    "elif DATASET == 'CC12M':\n",
    "    all_data = CC12mDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c265da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda'\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940a115",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24411b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses.clipLoss import ClipLoss\n",
    "from losses.cloobLoss import CLOOBLoss\n",
    "\n",
    "if MODEL == 'CLIP':\n",
    "    model = CLIPModel(\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    loss = ClipLoss()\n",
    "elif MODEL == 'CLOOB':\n",
    "    model = CLOOBModel(\n",
    "        device=DEVICE\n",
    "    )\n",
    "    config = model.get_config()\n",
    "    loss = CLOOBLoss(config['inv_tau'], config['scale_hopfield'])\n",
    "elif MODEL == 'VCLIP':\n",
    "    model = VariationalCLIPModel(\n",
    "        device=DEVICE\n",
    "    )\n",
    "    loss = None\n",
    "else:\n",
    "    model = AlignCLIPModel(\n",
    "        device=DEVICE\n",
    "    )\n",
    "    loss = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['LEARNING_RATE'],\n",
    "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=CONFIG['STEP_LR_STEP_SIZE'],\n",
    "    gamma=CONFIG['STEP_LR_GAMMA']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b50aa",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        criterion: torch.nn.Module, \n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    total_loss = 0.0\n",
    "    nan_count = 0\n",
    "\n",
    "    for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "        images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "    \n",
    "        # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "        if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "            logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        image_features, text_features = model(images)\n",
    "    \n",
    "        # Check for NaN in features\n",
    "        if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "            logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(image_features, text_features)\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            logger.warning(f\"NaN loss detected at batch {batch_idx}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        # If not NaN, then add to total loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / CONFIG['GRAD_ACCUMULATION_STEPS']\n",
    "        # Backward pass\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        has_nan_grads = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                has_nan_grads = True\n",
    "                logger.warning(f\"NaN gradient in {name}\")\n",
    "                break\n",
    "\n",
    "        if has_nan_grads:\n",
    "            logger.warning(f\"NaN gradients detected at batch {batch_idx}, skipping update\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # Gradient accumulation and optimization step\n",
    "        if (batch_idx + 1) % CONFIG['GRAD_ACCUMULATION_STEPS'] == 0:\n",
    "            # Clip gradients to prevent explosion\n",
    "            if CONFIG['CLIP_GRADIENTS']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                  'nan_count': nan_count\n",
    "            })\n",
    "\n",
    "\n",
    "        if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Handle remaining gradients if not accumulated evenly\n",
    "    if len(dataloader) % CONFIG['GRAD_ACCUMULATION_STEPS'] != 0:\n",
    "        if CONFIG['CLIP_GRADIENTS']:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9253e14",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        criterion: torch.nn.Module,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Evaluating\")\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "    \n",
    "            # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "                logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "        \n",
    "            image_features, text_features = model(images)\n",
    "\n",
    "            # Check for NaN in features\n",
    "            if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "                logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "        \n",
    "            loss = criterion(image_features, text_features)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                logger.warning(\"NaN in validation loss, skipping batch\")\n",
    "                continue\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "            if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632e2d4",
   "metadata": {},
   "source": [
    "# Full Train Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    filename: str,\n",
    "    model: ModelClass, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_losses: list[float],\n",
    "    val_losses: list[float],\n",
    "):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': CONFIG,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    logger.info(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def plot_losses(model_name: str, train_losses: list[float], val_losses: list[float]):\n",
    "    \"\"\"Plot training and validation losses.\"\"\"\n",
    "    if len(train_losses) == 0:\n",
    "        logger.warning(\"No losses to plot\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name.upper()} Training Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( \n",
    "    num_epochs: int,\n",
    "    model: ModelClass, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    criterion: torch.nn.Module\n",
    "):\n",
    "    \"\"\"Train model for specified epochs.\"\"\"\n",
    "    logger.info(f\"Starting training on {DEVICE} for {num_epochs} epochs...\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "    criterion\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Skip if losses are NaN\n",
    "        if torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss)):\n",
    "            logger.error(\"NaN loss detected! Stopping training.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "        logger.info(f\"Learning Rate adjusted to: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                f\"best_{MODEL}_model.pt\", \n",
    "                model,\n",
    "                optimizer, \n",
    "                train_losses,\n",
    "                val_losses\n",
    "            )\n",
    "            logger.info(f\"âœ“ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "\n",
    "        if CONFIG['USE_WANDB']:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "            })\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7928c",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ef514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use wandb? Resume Training?\n",
    "PROJECT_NAME = CONFIG['WANDB_PROJECT_NAME']\n",
    "USE_WANDB = CONFIG['USE_WANDB']\n",
    "RESUME_LOGGING = CONFIG['WANDB_PREVIOUS_RUN_ID'] is not None\n",
    "run_name = CONFIG['WANDB_RUN_NAME']\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login(key=WANDB_API_KEY)  # your wandb key\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = CONFIG['WANDB_PREVIOUS_RUN_ID']\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            id=run_id,\n",
    "            resume=\"must\",\n",
    "            project=PROJECT_NAME,\n",
    "        )\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name=run_name,\n",
    "            reinit=True,\n",
    "            project=PROJECT_NAME,\n",
    "            config=CONFIG\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c04d08",
   "metadata": {},
   "source": [
    "# Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadfc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train(\n",
    "    CONFIG[\"NUM_EPOCHS\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(MODEL, train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardelephant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
