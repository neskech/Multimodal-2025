{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eafb47b",
   "metadata": {},
   "source": [
    "# Initialization and Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c776d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all colab initialization code inside this block or make a copy of this notebook\n",
    "if IS_COLAB:   \n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "\n",
    "    github_token = \"\"\n",
    "    github_username = \"\" # Replace with your GitHub username\n",
    "    repository_url = f\"https://{github_username}:{github_token}@github.com/neskech/Multimodal-2025.git\"\n",
    "\n",
    "    !git clone {repository_url}\n",
    "\n",
    "    %cd Multimodal-2025\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee0a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ness/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/ness/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/ness/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-22 18:25:13,588 - INFO - JAX version 0.8.0 available.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import peft\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from Datasets.coco import CocoDataset\n",
    "from Datasets.cc12m import CC12mDataset\n",
    "from Datasets.cood import CoodDataset\n",
    "from Datasets.laion import LaionDataset\n",
    "from Models.clipModel import CLIPModel\n",
    "from Models.cloobModel import CLOOBModel\n",
    "from Models.vClipModel import VariationalCLIPModel\n",
    "from Models.alignClipModel import AlignCLIPModel\n",
    "from losses.clipLoss import ClipLoss\n",
    "from losses.cloobLoss import CLOOBLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd23854",
   "metadata": {},
   "outputs": [],
   "source": [
    "type Model = Literal['CLIP', 'CLOOB', 'ALIGN']\n",
    "type ModelClass = CLIPModel | CLOOBModel | AlignCLIPModel\n",
    "MODEL: Model = 'CLIP'\n",
    "\n",
    "type Dataset = Literal['COCO', 'COOD', 'CC12M', 'LAION']\n",
    "DATASET: Dataset = 'COOD'\n",
    "\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get WANDB Key (Use a .env file to store the key)\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.environ.get('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef7f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'NUM_EPOCHS': 20,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'LEARNING_RATE': 1e-5,\n",
    "    'WEIGHT_DECAY': 1e-1,\n",
    "\n",
    "    # Scheduler parameters\n",
    "    'STEP_LR_STEP_SIZE': 5,\n",
    "    'STEP_LR_GAMMA': 0.5,\n",
    "\n",
    "    # To avoid gradient explosion. Set to 1 to disable\n",
    "    'GRAD_ACCUMULATION_STEPS': 1,\n",
    "    # Clip gradients to avoid explosion\n",
    "    'CLIP_GRADIENTS': True,\n",
    "    'EMPTY_CACHE_AFTER_BATCH': True,\n",
    "\n",
    "    'DATA_DIR': '../Data',\n",
    "    'TRAIN_RATIO': 0.8,\n",
    "    'TOTAL_DATAPOINTS': 10_000,\n",
    "\n",
    "    'USE_LORA': False,\n",
    "\n",
    "    'USE_WANDB': True,\n",
    "    'WANDB_RUN_NAME': 'experiment_1',\n",
    "    'WANDB_PREVIOUS_RUN_ID': None, # set to None if not resuming\n",
    "    'WANDB_PROJECT_NAME': 'multimodal_2025',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf6745",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e96d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:25:14,105 - INFO - COOD dataset already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "num_train = int(CONFIG['TOTAL_DATAPOINTS'] * CONFIG['TRAIN_RATIO'])\n",
    "num_val = int(CONFIG['TOTAL_DATAPOINTS'] * (1.0 - CONFIG['TRAIN_RATIO']))\n",
    "\n",
    "if DATASET == 'COCO':\n",
    "    CocoDataset.download(download_script_path='../Datasets/download_coco.sh', data_dir=CONFIG['DATA_DIR'])\n",
    "    train = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='train2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_train\n",
    "    )\n",
    "    val = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='val2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_val\n",
    "    ) \n",
    "    collate_fn = CocoDataset.collate_function  \n",
    "elif DATASET == 'COOD':\n",
    "    CoodDataset.download(data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = CoodDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = CoodDataset.collate_function\n",
    "elif DATASET == 'LAION':\n",
    "    all_data = LaionDataset(\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = LaionDataset.collate_function\n",
    "elif DATASET == 'CC12M':\n",
    "    CC12mDataset.download(data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = CC12mDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = CC12mDataset.collate_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c265da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda',\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda',\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77a18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:25:14,187 - INFO - Training on 8000 samples, validating on 2000 samples.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Training on {len(train)} samples, validating on {len(val)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940a115",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24411b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'CLIP':\n",
    "    model = CLIPModel(DEVICE)\n",
    "    loss = ClipLoss()\n",
    "elif MODEL == 'CLOOB':\n",
    "    model = CLOOBModel(DEVICE)\n",
    "    config = model.get_config()\n",
    "    loss = CLOOBLoss(config['inv_tau'], config['scale_hopfield'])\n",
    "else:\n",
    "    model = AlignCLIPModel(DEVICE)\n",
    "    loss = None # TODO: Implement\n",
    "\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c402ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This only works if you apply it to some modules\n",
    "# See https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb \n",
    "# See https://huggingface.co/docs/peft/en/developer_guides/custom_models\n",
    "if CONFIG['USE_LORA']:\n",
    "   # Idek what any of this does\n",
    "    lora_config = peft.LoraConfig(\n",
    "        r=8, # Rank of the low-rank matrices\n",
    "        lora_alpha=16, # Scaling factor for LoRA updates\n",
    "       # target_modules=[\"model.visual.conv1\"], # Layers to apply LoRA to (e.g., in a Transformer)\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=peft.TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    model = peft.PeftModel(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6077737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['LEARNING_RATE'],\n",
    "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=CONFIG['STEP_LR_STEP_SIZE'],\n",
    "    gamma=CONFIG['STEP_LR_GAMMA']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b50aa",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e81901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        criterion: torch.nn.Module, \n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    total_loss = 0.0\n",
    "    nan_count = 0\n",
    "\n",
    "    for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "        images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "        images = images.float()\n",
    "    \n",
    "        # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "        if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "            logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        image_features = model.encode_image_tensors(images)\n",
    "        text_features = model.encode_text_tokens(text_tokens)\n",
    "    \n",
    "        # Check for NaN in features\n",
    "        if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "            logger.warning(f\"NaN in features at batch {batch_idx}: Image features stats - min={image_features.min()}, max={image_features.max()}, mean={image_features.mean()}; Text features stats - min={text_features.min()}, max={text_features.max()}, mean={text_features.mean()}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(image_features, text_features)\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            logger.warning(f\"NaN loss detected at batch {batch_idx}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        # If not NaN, then add to total loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / CONFIG['GRAD_ACCUMULATION_STEPS']\n",
    "        # Backward pass\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        has_nan_grads = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                has_nan_grads = True\n",
    "                logger.warning(f\"NaN gradient in {name}\")\n",
    "                break\n",
    "\n",
    "        if has_nan_grads:\n",
    "            logger.warning(f\"NaN gradients detected at batch {batch_idx}, skipping update\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # Gradient accumulation and optimization step\n",
    "        if (batch_idx + 1) % CONFIG['GRAD_ACCUMULATION_STEPS'] == 0:\n",
    "            # Clip gradients to prevent explosion\n",
    "            if CONFIG['CLIP_GRADIENTS']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'nan_count': nan_count\n",
    "            })\n",
    "\n",
    "\n",
    "        if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Handle remaining gradients if not accumulated evenly\n",
    "    if len(dataloader) % CONFIG['GRAD_ACCUMULATION_STEPS'] != 0:\n",
    "        if CONFIG['CLIP_GRADIENTS']:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9253e14",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "728ec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        criterion: torch.nn.Module,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Evaluating\")\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "            images = images.float()\n",
    "    \n",
    "            # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "                logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "            \n",
    "            image_features = model.encode_image_tensors(images)\n",
    "            text_features = model.encode_text_tokens(text_tokens)\n",
    "            \n",
    "            # Check for NaN in features\n",
    "            if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "                logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "        \n",
    "            loss = criterion(image_features, text_features)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                logger.warning(\"NaN in validation loss, skipping batch\")\n",
    "                continue\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "            if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632e2d4",
   "metadata": {},
   "source": [
    "# Full Train Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49e6c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    filename: str,\n",
    "    model: ModelClass, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_losses: list[float],\n",
    "    val_losses: list[float],\n",
    "):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': CONFIG,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    logger.info(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def plot_losses(model_name: str, train_losses: list[float], val_losses: list[float]):\n",
    "    \"\"\"Plot training and validation losses.\"\"\"\n",
    "    if len(train_losses) == 0:\n",
    "        logger.warning(\"No losses to plot\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name.upper()} Training Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d94e147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( \n",
    "    num_epochs: int,\n",
    "    model: ModelClass, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    criterion: torch.nn.Module\n",
    "):\n",
    "    \"\"\"Train model for specified epochs.\"\"\"\n",
    "    logger.info(f\"Starting training on {DEVICE} for {num_epochs} epochs...\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "    criterion\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Skip if losses are NaN\n",
    "        if torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss)):\n",
    "            logger.error(\"NaN loss detected! Stopping training.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "        logger.info(f\"Learning Rate adjusted to: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                f\"best_{MODEL}_model.pt\", \n",
    "                model,\n",
    "                optimizer, \n",
    "                train_losses,\n",
    "                val_losses\n",
    "            )\n",
    "            logger.info(f\"✓ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "\n",
    "        if CONFIG['USE_WANDB']:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "            })\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7928c",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "738ef514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcmellor\u001b[0m (\u001b[33mcmellor-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ness/School/Senior/Multimodal-2025/Notebooks/wandb/run-20251022_182516-r4d5pqvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/r4d5pqvf' target=\"_blank\">experiment_1</a></strong> to <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025' target=\"_blank\">https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/r4d5pqvf' target=\"_blank\">https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/r4d5pqvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use wandb? Resume Training?\n",
    "PROJECT_NAME = CONFIG['WANDB_PROJECT_NAME']\n",
    "USE_WANDB = CONFIG['USE_WANDB']\n",
    "RESUME_LOGGING = CONFIG['WANDB_PREVIOUS_RUN_ID'] is not None\n",
    "run_name = CONFIG['WANDB_RUN_NAME']\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login(key=WANDB_API_KEY)  # your wandb key\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = CONFIG['WANDB_PREVIOUS_RUN_ID']\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            id=run_id,\n",
    "            resume=\"must\",\n",
    "            project=PROJECT_NAME,\n",
    "        )\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name=run_name,\n",
    "            reinit=True,\n",
    "            project=PROJECT_NAME,\n",
    "            config=CONFIG\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c04d08",
   "metadata": {},
   "source": [
    "# Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edadfc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 18:25:17,251 - INFO - Starting training on cuda for 20 epochs...\n",
      "2025-10-22 18:25:17,251 - INFO - \n",
      "Epoch 1/20\n",
      "Training Epoch:   0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch:  11%|█         | 28/250 [00:12<01:40,  2.20it/s, loss=0.884, nan_count=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_losses, val_losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNUM_EPOCHS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(num_epochs, model, train_loader, val_loader, optimizer, scheduler, criterion)\u001b[39m\n\u001b[32m     18\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m\u001b[49m\u001b[43mcriterion\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     train_losses.append(train_loss)\n\u001b[32m     28\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m     10\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     11\u001b[39m nan_count = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/Notebooks/../Datasets/cood.py:47\u001b[39m, in \u001b[36mCoodDataset.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     44\u001b[39m caption = \u001b[38;5;28mself\u001b[39m.data[index][\u001b[33m\"\u001b[39m\u001b[33mcaption\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     46\u001b[39m image = Image.open(image_path).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m imageTensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tokenize:\n\u001b[32m     50\u001b[39m     caption = clip.tokenize(caption, truncate=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torchvision/transforms/transforms.py:354\u001b[39m, in \u001b[36mResize.forward\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m    347\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    352\u001b[39m \u001b[33;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torchvision/transforms/functional.py:477\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    475\u001b[39m         warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    476\u001b[39m     pil_interpolation = pil_modes_mapping[interpolation]\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F_t.resize(img, size=output_size, interpolation=interpolation.value, antialias=antialias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torchvision/transforms/_functional_pil.py:253\u001b[39m, in \u001b[36mresize\u001b[39m\u001b[34m(img, size, interpolation)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) == \u001b[32m2\u001b[39m):\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/PIL/Image.py:2304\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2292\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2294\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2295\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2296\u001b[39m         )\n\u001b[32m   2297\u001b[39m         box = (\n\u001b[32m   2298\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2299\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2300\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2301\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2302\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x73cf0fa702f0>> (for post_run_cell), with arguments args (<ExecutionResult object at 73ceacb492e0, execution_count=15 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 73ceacb48f70, raw_cell=\"train_losses, val_losses = train(\n",
      "    CONFIG[\"NUM_..\" transformed_cell=\"train_losses, val_losses = train(\n",
      "    CONFIG[\"NUM_..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://wsl%2Bubuntu/home/ness/School/Senior/Multimodal-2025/Notebooks/finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:595\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    594\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface.py:818\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    817\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_shared.py:296\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    295\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/interface/interface_sock.py:43\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, local)\u001b[39m\n\u001b[32m     41\u001b[39m request = spb.ServerRequest()\n\u001b[32m     42\u001b[39m request.record_publish.CopyFrom(record)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/streams.py:386\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    376\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    377\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    385\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.3-linux-x86_64-gnu/lib/python3.13/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(\n",
    "    CONFIG[\"NUM_EPOCHS\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(MODEL, train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardelephant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
