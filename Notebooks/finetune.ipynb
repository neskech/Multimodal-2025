{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eafb47b",
   "metadata": {},
   "source": [
    "# Initialization and Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c776d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_COLAB:   \n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "\n",
    "    github_token = \"\"\n",
    "    github_username = \"\" # Replace with your GitHub username\n",
    "    repository_url = f\"https://{github_username}:{github_token}@github.com/neskech/Multimodal-2025.git\"\n",
    "\n",
    "    !git clone {repository_url}\n",
    "    \n",
    "\n",
    "    %cd Multimodal-2025\n",
    "    !git checkout baselines\n",
    "    !git submodule update --init --recursive\n",
    "    \n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsumacpro/CardElephant/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/tsumacpro/CardElephant/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/tsumacpro/CardElephant/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-23 22:55:30,715 - INFO - JAX version 0.8.0 available.\n"
     ]
    }
   ],
   "source": [
    "if not IS_COLAB: # colab does not seem to support these\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %reload_ext autoreload\n",
    "\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import peft\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal, Union\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from Datasets.coco import CocoDataset\n",
    "from Datasets.cc12m import CC12mDataset\n",
    "from Datasets.cood import CoodDataset\n",
    "from Datasets.laion import LaionDataset\n",
    "from Models.clipModel import CLIPModel\n",
    "from Models.cloobModel import CLOOBModel\n",
    "from Models.vClipModel import VariationalCLIPModel\n",
    "from Models.alignClipModel import AlignCLIPModel\n",
    "from losses.clipLoss import ClipLoss\n",
    "from losses.cloobLoss import CLOOBLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd23854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type aliases for compatibility with older Python versions\n",
    "Model = Literal['CLIP', 'CLOOB', 'ALIGN']\n",
    "ModelClass = Union[CLIPModel, CLOOBModel, AlignCLIPModel]\n",
    "MODEL: Model = 'CLOOB'\n",
    "\n",
    "Dataset = Literal['COCO', 'COOD', 'CC12M', 'LAION']\n",
    "DATASET: Dataset = 'COCO'\n",
    "\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get WANDB Key (Use a .env file to store the key)\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.environ.get('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef7f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'NUM_EPOCHS': 10,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'LEARNING_RATE': 1e-5,\n",
    "    'WEIGHT_DECAY': 1e-2,\n",
    "\n",
    "    # Scheduler parameters\n",
    "    'STEP_LR_STEP_SIZE': 5,\n",
    "    'STEP_LR_GAMMA': 0.5,\n",
    "\n",
    "    # To avoid gradient explosion. Set to 1 to disable\n",
    "    'GRAD_ACCUMULATION_STEPS': 1,\n",
    "    # Clip gradients to avoid explosion\n",
    "    'CLIP_GRADIENTS': True,\n",
    "    'EMPTY_CACHE_AFTER_BATCH': False,\n",
    "\n",
    "    'DATA_DIR': '../Data',\n",
    "    'TRAIN_RATIO': 0.8,\n",
    "    'TOTAL_DATAPOINTS': 10_000,\n",
    "\n",
    "    'USE_LORA': False,\n",
    "\n",
    "    'USE_WANDB': True,\n",
    "    'WANDB_RUN_NAME': 'CLIP_COCO',\n",
    "    'WANDB_PREVIOUS_RUN_ID': None, # set to None if not resuming\n",
    "    'WANDB_PROJECT_NAME': 'multimodal_2025',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf6745",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e96d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 22:55:31,300 - INFO - COCO dataset already exists. Skipping download.\n",
      "2025-10-23 22:55:31,301 - INFO - Loading COCO train2017 dataset...\n",
      "2025-10-23 22:55:32,700 - INFO - Loading COCO val2017 dataset...\n"
     ]
    }
   ],
   "source": [
    "num_train = int(CONFIG['TOTAL_DATAPOINTS'] * CONFIG['TRAIN_RATIO'])\n",
    "num_val = int(CONFIG['TOTAL_DATAPOINTS'] * (1.0 - CONFIG['TRAIN_RATIO']))\n",
    "\n",
    "if DATASET == 'COCO':\n",
    "    CocoDataset.download(download_script_path='../Datasets/download_coco.sh', data_dir=CONFIG['DATA_DIR'])\n",
    "    train = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='train2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_train\n",
    "    )\n",
    "    val = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='val2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_val\n",
    "    ) \n",
    "    collate_fn = CocoDataset.collate_function  \n",
    "elif DATASET == 'COOD':\n",
    "    CoodDataset.download(data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = CoodDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = CoodDataset.collate_function\n",
    "elif DATASET == 'LAION':\n",
    "    LaionDataset.download(max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = LaionDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = LaionDataset.collate_function\n",
    "elif DATASET == 'CC12M':\n",
    "    CC12mDataset.download(max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = CC12mDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = CC12mDataset.collate_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c265da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda',\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda',\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77a18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 22:55:32,839 - INFO - Training on 8000 samples, validating on 1999 samples.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Training on {len(train)} samples, validating on {len(val)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940a115",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24411b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'CLIP':\n",
    "    model = CLIPModel(DEVICE)\n",
    "    model.freeze_for_finetuning()\n",
    "    loss = ClipLoss()\n",
    "elif MODEL == 'CLOOB':\n",
    "    model = CLOOBModel(DEVICE)\n",
    "    config = model.get_config()\n",
    "    loss = CLOOBLoss(config['inv_tau'], config['scale_hopfield'], device=DEVICE)\n",
    "else:\n",
    "    model = AlignCLIPModel(DEVICE)\n",
    "    loss = None # TODO: Implement\n",
    "\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c402ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This only works if you apply it to some modules\n",
    "# See https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb \n",
    "# See https://huggingface.co/docs/peft/en/developer_guides/custom_models\n",
    "if CONFIG['USE_LORA']:\n",
    "   # Idek what any of this does\n",
    "    lora_config = peft.LoraConfig(\n",
    "        r=8, # Rank of the low-rank matrices\n",
    "        lora_alpha=16, # Scaling factor for LoRA updates\n",
    "       # target_modules=[\"model.visual.conv1\"], # Layers to apply LoRA to (e.g., in a Transformer)\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=peft.TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    model = peft.PeftModel(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6077737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['LEARNING_RATE'],\n",
    "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
    ")\n",
    "\n",
    "# Warmup scheduler: Linear increase from 0 to target_lr\n",
    "warmup_epochs = 5  # Number of epochs for warmup\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs)\n",
    "\n",
    "# Decay scheduler: Cosine annealing after warmup\n",
    "decay_epochs = 20  # Number of epochs for decay\n",
    "decay_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=decay_epochs)\n",
    "\n",
    "# Combine them using SequentialLR\n",
    "# The schedulers will be applied sequentially\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, decay_scheduler], milestones=[warmup_epochs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b50aa",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e81901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        criterion: torch.nn.Module, \n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    total_loss = 0.0\n",
    "    nan_count = 0\n",
    "\n",
    "    for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "        images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "        images = images.float()\n",
    "    \n",
    "        # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "        if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "            logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        image_features = model.encode_image_tensors(images)\n",
    "        text_features = model.encode_text_tokens(text_tokens)\n",
    "    \n",
    "        # Check for NaN in features\n",
    "        if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "            logger.warning(f\"NaN in features at batch {batch_idx}: Image features stats - min={image_features.min()}, max={image_features.max()}, mean={image_features.mean()}; Text features stats - min={text_features.min()}, max={text_features.max()}, mean={text_features.mean()}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(image_features, text_features)\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            logger.warning(f\"NaN loss detected at batch {batch_idx}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        # If not NaN, then add to total loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / CONFIG['GRAD_ACCUMULATION_STEPS']\n",
    "        # Backward pass\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        has_nan_grads = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                has_nan_grads = True\n",
    "                logger.warning(f\"NaN gradient in {name}\")\n",
    "                break\n",
    "\n",
    "        if has_nan_grads:\n",
    "            logger.warning(f\"NaN gradients detected at batch {batch_idx}, skipping update\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # Gradient accumulation and optimization step\n",
    "        if (batch_idx + 1) % CONFIG['GRAD_ACCUMULATION_STEPS'] == 0:\n",
    "            # Clip gradients to prevent explosion\n",
    "            if CONFIG['CLIP_GRADIENTS']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'nan_count': nan_count\n",
    "            })\n",
    "\n",
    "\n",
    "        if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Handle remaining gradients if not accumulated evenly\n",
    "    if len(dataloader) % CONFIG['GRAD_ACCUMULATION_STEPS'] != 0:\n",
    "        if CONFIG['CLIP_GRADIENTS']:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9253e14",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "728ec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        criterion: torch.nn.Module,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Evaluating\")\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "            images = images.float()\n",
    "    \n",
    "            # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "                logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "            \n",
    "            image_features = model.encode_image_tensors(images)\n",
    "            text_features = model.encode_text_tokens(text_tokens)\n",
    "            \n",
    "            # Check for NaN in features\n",
    "            if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "                logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "        \n",
    "            loss = criterion(image_features, text_features)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                logger.warning(\"NaN in validation loss, skipping batch\")\n",
    "                continue\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "            if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632e2d4",
   "metadata": {},
   "source": [
    "# Full Train Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e6c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    filename: str,\n",
    "    model: ModelClass, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_losses: list[float],\n",
    "    val_losses: list[float],\n",
    "):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': CONFIG,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    logger.info(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def plot_losses(model_name: str, train_losses: list[float], val_losses: list[float]):\n",
    "    \"\"\"Plot training and validation losses.\"\"\"\n",
    "    if len(train_losses) == 0:\n",
    "        logger.warning(\"No losses to plot\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name.upper()} Training Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d94e147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( \n",
    "    num_epochs: int,\n",
    "    model: ModelClass, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    criterion: torch.nn.Module\n",
    "):\n",
    "    \"\"\"Train model for specified epochs.\"\"\"\n",
    "    logger.info(f\"Starting training on {DEVICE} for {num_epochs} epochs...\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            criterion\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Skip if losses are NaN\n",
    "        if torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss)):\n",
    "            logger.error(\"NaN loss detected! Stopping training.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "        logger.info(f\"Learning Rate adjusted to: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                f\"best_{MODEL}_model_on_{DATASET}.pt\", \n",
    "                model,\n",
    "                optimizer, \n",
    "                train_losses,\n",
    "                val_losses\n",
    "            )\n",
    "            logger.info(f\"âœ“ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "\n",
    "        if CONFIG['USE_WANDB']:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "            })\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7928c",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "738ef514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CLIP_COCO</strong> at: <a href='https://wandb.ai/multimodal_2025/multimodal_2025/runs/y9u6s9oc' target=\"_blank\">https://wandb.ai/multimodal_2025/multimodal_2025/runs/y9u6s9oc</a><br> View project at: <a href='https://wandb.ai/multimodal_2025/multimodal_2025' target=\"_blank\">https://wandb.ai/multimodal_2025/multimodal_2025</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251023_230151-y9u6s9oc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tsumacpro/CardElephant/Notebooks/wandb/run-20251023_230254-jgfo4ulr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/glwm7988' target=\"_blank\">CLIP_LAION</a></strong> to <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/multimodal_2025/multimodal_2025' target=\"_blank\">https://wandb.ai/multimodal_2025/multimodal_2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/multimodal_2025/multimodal_2025/runs/jgfo4ulr' target=\"_blank\">https://wandb.ai/multimodal_2025/multimodal_2025/runs/jgfo4ulr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use wandb? Resume Training?\n",
    "PROJECT_NAME = CONFIG['WANDB_PROJECT_NAME']\n",
    "USE_WANDB = CONFIG['USE_WANDB']\n",
    "RESUME_LOGGING = CONFIG['WANDB_PREVIOUS_RUN_ID'] is not None\n",
    "run_name = CONFIG['WANDB_RUN_NAME']\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = CONFIG['WANDB_PREVIOUS_RUN_ID']\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            id=run_id,\n",
    "            resume=\"must\",\n",
    "            project=PROJECT_NAME,\n",
    "            entity=\"multimodal_2025\",\n",
    "        )\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name=run_name,\n",
    "            reinit=True,\n",
    "            project=PROJECT_NAME,\n",
    "            config=CONFIG,\n",
    "            entity=\"multimodal_2025\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c04d08",
   "metadata": {},
   "source": [
    "# Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edadfc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 23:02:57,958 - INFO - Starting training on mps for 10 epochs...\n",
      "2025-10-23 23:02:57,959 - INFO - \n",
      "Epoch 1/10\n",
      "Training Epoch:   1%|          | 2/250 [00:14<30:53,  7.48s/it, loss=-13.8, nan_count=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_losses, val_losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNUM_EPOCHS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(num_epochs, model, train_loader, val_loader, optimizer, scheduler, criterion)\u001b[39m\n\u001b[32m     18\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m train_losses.append(train_loss)\n\u001b[32m     28\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     23\u001b[39m image_features = model.encode_image_tensors(images)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m text_features = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_text_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check for NaN in features\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(image_features).any() \u001b[38;5;129;01mor\u001b[39;00m torch.isnan(text_features).any():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/Notebooks/../Models/cloobModel.py:111\u001b[39m, in \u001b[36mCLOOBModel.encode_text_tokens\u001b[39m\u001b[34m(self, text_tokens, requires_grad, normalize)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03mEncode tokenized text to embeddings.\u001b[39;00m\n\u001b[32m     99\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m    Shape: [batch_size, 512]\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m requires_grad:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     text_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/Notebooks/../Models/../cloob-training/cloob_training/model_pt.py:109\u001b[39m, in \u001b[36mTextEncoder.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    107\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pos_embed(x)\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     x = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m x = x[:, \u001b[32m0\u001b[39m]\n\u001b[32m    111\u001b[39m x = \u001b[38;5;28mself\u001b[39m.proj(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/Notebooks/../Models/../cloob-training/cloob_training/model_pt.py:82\u001b[39m, in \u001b[36mTransformerEncoderLayer.__call__\u001b[39m\u001b[34m(self, x, padding_mask)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, padding_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     x = x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.ff(x)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/CardElephant/Notebooks/../Models/../cloob-training/cloob_training/model_pt.py:52\u001b[39m, in \u001b[36mSelfAttention.forward\u001b[39m\u001b[34m(self, x, padding_mask)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     mask = padding_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]  \u001b[38;5;66;03m# This is wrong! It should be [:, None, None, :].\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     attn_logits = torch.where(mask, attn_logits, \u001b[43mattn_logits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1e30\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     53\u001b[39m attn_weights = attn_logits.softmax(-\u001b[32m1\u001b[39m)\n\u001b[32m     54\u001b[39m attn = torch.einsum(\u001b[33m'\u001b[39m\u001b[33m...htT,...Thd->...thd\u001b[39m\u001b[33m'\u001b[39m, attn_weights, v)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(\n",
    "    CONFIG[\"NUM_EPOCHS\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(MODEL, train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CardElephant (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
