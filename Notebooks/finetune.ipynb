{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eafb47b",
   "metadata": {},
   "source": [
    "# Initialization and Config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c776d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7833c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all colab initialization code inside this block or make a copy of this notebook\n",
    "if IS_COLAB:   \n",
    "    from google.colab import userdata\n",
    "    import os\n",
    "\n",
    "    github_token = \"\"\n",
    "    github_username = \"\" # Replace with your GitHub username\n",
    "    repository_url = f\"https://{github_username}:{github_token}@github.com/neskech/Multimodal-2025.git\"\n",
    "\n",
    "    !git clone {repository_url}\n",
    "\n",
    "    %cd Multimodal-2025\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee0a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ness/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/ness/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/ness/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-23 22:28:52,749 - INFO - JAX version 0.8.0 available.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "import wandb\n",
    "import os\n",
    "import sys\n",
    "import peft\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from Datasets.coco import CocoDataset\n",
    "from Datasets.cc12m import CC12mDataset\n",
    "from Datasets.cood import CoodDataset\n",
    "from Datasets.laion import LaionDataset\n",
    "from Models.clipModel import CLIPModel\n",
    "from Models.cloobModel import CLOOBModel\n",
    "from Models.vClipModel import VariationalCLIPModel\n",
    "from Models.alignClipModel import AlignCLIPModel\n",
    "from losses.clipLoss import ClipLoss\n",
    "from losses.cloobLoss import CLOOBLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd23854",
   "metadata": {},
   "outputs": [],
   "source": [
    "type Model = Literal['CLIP', 'CLOOB', 'ALIGN']\n",
    "type ModelClass = CLIPModel | CLOOBModel | AlignCLIPModel\n",
    "MODEL: Model = 'CLOOB'\n",
    "\n",
    "type Dataset = Literal['COCO', 'COOD', 'CC12M', 'LAION']\n",
    "DATASET: Dataset = 'LAION'\n",
    "\n",
    "DEVICE = None\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "elif torch.mps.is_available():\n",
    "    DEVICE = 'mps'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get WANDB Key (Use a .env file to store the key)\n",
    "load_dotenv()\n",
    "WANDB_API_KEY = os.environ.get('WANDB_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef7f680",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'NUM_EPOCHS': 10,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'LEARNING_RATE': 1e-5,\n",
    "    'WEIGHT_DECAY': 1e-2,\n",
    "\n",
    "    # Scheduler parameters\n",
    "    'STEP_LR_STEP_SIZE': 5,\n",
    "    'STEP_LR_GAMMA': 0.5,\n",
    "\n",
    "    # To avoid gradient explosion. Set to 1 to disable\n",
    "    'GRAD_ACCUMULATION_STEPS': 1,\n",
    "    # Clip gradients to avoid explosion\n",
    "    'CLIP_GRADIENTS': True,\n",
    "    'EMPTY_CACHE_AFTER_BATCH': False,\n",
    "\n",
    "    'DATA_DIR': '../Data',\n",
    "    'TRAIN_RATIO': 0.8,\n",
    "    'TOTAL_DATAPOINTS': 10_000,\n",
    "\n",
    "    'USE_LORA': False,\n",
    "\n",
    "    'USE_WANDB': True,\n",
    "    'WANDB_RUN_NAME': 'CLIP_LAION',\n",
    "    'WANDB_PREVIOUS_RUN_ID': None, # set to None if not resuming\n",
    "    'WANDB_PROJECT_NAME': 'multimodal_2025',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf6745",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e96d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 22:28:53,806 - INFO - LAION dataset samples already downloaded.\n"
     ]
    }
   ],
   "source": [
    "num_train = int(CONFIG['TOTAL_DATAPOINTS'] * CONFIG['TRAIN_RATIO'])\n",
    "num_val = int(CONFIG['TOTAL_DATAPOINTS'] * (1.0 - CONFIG['TRAIN_RATIO']))\n",
    "\n",
    "if DATASET == 'COCO':\n",
    "    CocoDataset.download(download_script_path='../Datasets/download_coco.sh', data_dir=CONFIG['DATA_DIR'])\n",
    "    train = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='train2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_train\n",
    "    )\n",
    "    val = CocoDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        split='val2017',\n",
    "        tokenize=True,\n",
    "        max_samples=num_val\n",
    "    ) \n",
    "    collate_fn = CocoDataset.collate_function  \n",
    "elif DATASET == 'COOD':\n",
    "    CoodDataset.download(data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = CoodDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = CoodDataset.collate_function\n",
    "elif DATASET == 'LAION':\n",
    "    LaionDataset.download(max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = LaionDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = LaionDataset.collate_function\n",
    "elif DATASET == 'CC12M':\n",
    "    CC12mDataset.download(max_samples=CONFIG['TOTAL_DATAPOINTS'], data_dir=CONFIG['DATA_DIR'])\n",
    "    all_data = CC12mDataset(\n",
    "        data_dir=CONFIG['DATA_DIR'],\n",
    "        tokenize=True,\n",
    "        max_samples=CONFIG['TOTAL_DATAPOINTS']\n",
    "    )\n",
    "    train = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(0, num_train)\n",
    "    )\n",
    "    val = torch.utils.data.Subset(\n",
    "        all_data,\n",
    "        range(num_train, CONFIG['TOTAL_DATAPOINTS'])\n",
    "    )\n",
    "    collate_fn = CC12mDataset.collate_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c265da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda',\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val,\n",
    "    CONFIG['BATCH_SIZE'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=DEVICE == 'cuda',\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77a18b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 22:28:53,865 - INFO - Training on 8000 samples, validating on 2000 samples.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Training on {len(train)} samples, validating on {len(val)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a940a115",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24411b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL == 'CLIP':\n",
    "    model = CLIPModel(DEVICE)\n",
    "    model.freeze_for_finetuning()\n",
    "    loss = ClipLoss()\n",
    "elif MODEL == 'CLOOB':\n",
    "    model = CLOOBModel(DEVICE)\n",
    "    config = model.get_config()\n",
    "    loss = CLOOBLoss(config['inv_tau'], config['scale_hopfield'], device=DEVICE)\n",
    "else:\n",
    "    model = AlignCLIPModel(DEVICE)\n",
    "    loss = None # TODO: Implement\n",
    "\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This only works if you apply it to some modules\n",
    "# See https://github.com/huggingface/peft/blob/main/examples/multilayer_perceptron/multilayer_perceptron_lora.ipynb \n",
    "# See https://huggingface.co/docs/peft/en/developer_guides/custom_models\n",
    "if CONFIG['USE_LORA']:\n",
    "   # Idek what any of this does\n",
    "    lora_config = peft.LoraConfig(\n",
    "        r=8, # Rank of the low-rank matrices\n",
    "        lora_alpha=16, # Scaling factor for LoRA updates\n",
    "       # target_modules=[\"model.visual.conv1\"], # Layers to apply LoRA to (e.g., in a Transformer)\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=peft.TaskType.FEATURE_EXTRACTION\n",
    "    )\n",
    "    model = peft.PeftModel(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6077737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['LEARNING_RATE'],\n",
    "    weight_decay=CONFIG['WEIGHT_DECAY']\n",
    ")\n",
    "\n",
    "# Warmup scheduler: Linear increase from 0 to target_lr\n",
    "warmup_epochs = 5  # Number of epochs for warmup\n",
    "warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_epochs)\n",
    "\n",
    "# Decay scheduler: Cosine annealing after warmup\n",
    "decay_epochs = 20  # Number of epochs for decay\n",
    "decay_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=decay_epochs)\n",
    "\n",
    "# Combine them using SequentialLR\n",
    "# The schedulers will be applied sequentially\n",
    "scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, schedulers=[warmup_scheduler, decay_scheduler], milestones=[warmup_epochs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b50aa",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer, \n",
    "        criterion: torch.nn.Module, \n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    total_loss = 0.0\n",
    "    nan_count = 0\n",
    "\n",
    "    for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "        images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "        images = images.float()\n",
    "    \n",
    "        # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "        if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "            logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        image_features = model.encode_image_tensors(images)\n",
    "        text_features = model.encode_text_tokens(text_tokens)\n",
    "    \n",
    "        # Check for NaN in features\n",
    "        if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "            logger.warning(f\"NaN in features at batch {batch_idx}: Image features stats - min={image_features.min()}, max={image_features.max()}, mean={image_features.mean()}; Text features stats - min={text_features.min()}, max={text_features.max()}, mean={text_features.mean()}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        loss = criterion(image_features, text_features)\n",
    "\n",
    "        # Check for NaN in loss\n",
    "        if torch.isnan(loss):\n",
    "            logger.warning(f\"NaN loss detected at batch {batch_idx}\")\n",
    "            nan_count += 1\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "        \n",
    "        # If not NaN, then add to total loss\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Scale loss for gradient accumulation\n",
    "        scaled_loss = loss / CONFIG['GRAD_ACCUMULATION_STEPS']\n",
    "        # Backward pass\n",
    "        scaled_loss.backward()\n",
    "\n",
    "        has_nan_grads = False\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None and torch.isnan(param.grad).any():\n",
    "                has_nan_grads = True\n",
    "                logger.warning(f\"NaN gradient in {name}\")\n",
    "                break\n",
    "\n",
    "        if has_nan_grads:\n",
    "            logger.warning(f\"NaN gradients detected at batch {batch_idx}, skipping update\")\n",
    "            optimizer.zero_grad()\n",
    "            continue\n",
    "\n",
    "        # Gradient accumulation and optimization step\n",
    "        if (batch_idx + 1) % CONFIG['GRAD_ACCUMULATION_STEPS'] == 0:\n",
    "            # Clip gradients to prevent explosion\n",
    "            if CONFIG['CLIP_GRADIENTS']:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': total_loss / (batch_idx + 1),\n",
    "                'nan_count': nan_count\n",
    "            })\n",
    "\n",
    "\n",
    "        if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    # Handle remaining gradients if not accumulated evenly\n",
    "    if len(dataloader) % CONFIG['GRAD_ACCUMULATION_STEPS'] != 0:\n",
    "        if CONFIG['CLIP_GRADIENTS']:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9253e14",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ec899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "        model: ModelClass, \n",
    "        dataloader: torch.utils.data.DataLoader, \n",
    "        criterion: torch.nn.Module,\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Evaluating\")\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, text_tokens) in enumerate(progress_bar):\n",
    "            images, text_tokens = images.to(DEVICE), text_tokens.to(DEVICE)\n",
    "            images = images.float()\n",
    "    \n",
    "            # Check for NaN in input (laion gives NAN's if it. can't load images)\n",
    "            if torch.isnan(images).any() or torch.isnan(text_tokens).any():\n",
    "                logger.warning(f\"NaN in input batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "            \n",
    "            image_features = model.encode_image_tensors(images)\n",
    "            text_features = model.encode_text_tokens(text_tokens)\n",
    "            \n",
    "            # Check for NaN in features\n",
    "            if torch.isnan(image_features).any() or torch.isnan(text_features).any():\n",
    "                logger.warning(f\"NaN in features at batch {batch_idx}\")\n",
    "                optimizer.zero_grad()\n",
    "                continue\n",
    "        \n",
    "            loss = criterion(image_features, text_features)\n",
    "\n",
    "            # Check for NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                logger.warning(\"NaN in validation loss, skipping batch\")\n",
    "                continue\n",
    "        \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': total_loss / (batch_idx + 1)})\n",
    "\n",
    "            if CONFIG['EMPTY_CACHE_AFTER_BATCH']:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    epoch_loss = total_loss / len(dataloader)\n",
    "    return epoch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632e2d4",
   "metadata": {},
   "source": [
    "# Full Train Eval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(\n",
    "    filename: str,\n",
    "    model: ModelClass, \n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    train_losses: list[float],\n",
    "    val_losses: list[float],\n",
    "):\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'config': CONFIG,\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    logger.info(f\"Checkpoint saved: {filename}\")\n",
    "\n",
    "def plot_losses(model_name: str, train_losses: list[float], val_losses: list[float]):\n",
    "    \"\"\"Plot training and validation losses.\"\"\"\n",
    "    if len(train_losses) == 0:\n",
    "        logger.warning(\"No losses to plot\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Val Loss', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_name.upper()} Training Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( \n",
    "    num_epochs: int,\n",
    "    model: ModelClass, \n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    optimizer: torch.optim.Optimizer, \n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    criterion: torch.nn.Module\n",
    "):\n",
    "    \"\"\"Train model for specified epochs.\"\"\"\n",
    "    logger.info(f\"Starting training on {DEVICE} for {num_epochs} epochs...\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        logger.info(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            criterion\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        logger.info(f\"Train Loss: {train_loss:.6f}\")\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate(\n",
    "            model,\n",
    "            val_loader,\n",
    "            criterion\n",
    "        )\n",
    "        val_losses.append(val_loss)\n",
    "        logger.info(f\"Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "        # Skip if losses are NaN\n",
    "        if torch.isnan(torch.tensor(train_loss)) or torch.isnan(torch.tensor(val_loss)):\n",
    "            logger.error(\"NaN loss detected! Stopping training.\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "        logger.info(f\"Learning Rate adjusted to: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_checkpoint(\n",
    "                f\"best_{MODEL}_model_on_{DATASET}.pt\", \n",
    "                model,\n",
    "                optimizer, \n",
    "                train_losses,\n",
    "                val_losses\n",
    "            )\n",
    "            logger.info(f\"âœ“ Saved best model (val_loss: {val_loss:.6f})\")\n",
    "\n",
    "        if CONFIG['USE_WANDB']:\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'learning_rate': scheduler.get_last_lr()[0],\n",
    "            })\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c7928c",
   "metadata": {},
   "source": [
    "# WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738ef514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ness/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcmellor\u001b[0m (\u001b[33mcmellor-carnegie-mellon-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ness/School/Senior/Multimodal-2025/Notebooks/wandb/run-20251023_222652-hthjjnsi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/hthjjnsi' target=\"_blank\">CLIP_LAION</a></strong> to <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025' target=\"_blank\">https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/hthjjnsi' target=\"_blank\">https://wandb.ai/cmellor-carnegie-mellon-university/multimodal_2025/runs/hthjjnsi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use wandb? Resume Training?\n",
    "PROJECT_NAME = CONFIG['WANDB_PROJECT_NAME']\n",
    "USE_WANDB = CONFIG['USE_WANDB']\n",
    "RESUME_LOGGING = CONFIG['WANDB_PREVIOUS_RUN_ID'] is not None\n",
    "run_name = CONFIG['WANDB_RUN_NAME']\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb.login(key=WANDB_API_KEY)  # your wandb key\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = CONFIG['WANDB_PREVIOUS_RUN_ID']\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            id=run_id,\n",
    "            resume=\"must\",\n",
    "            project=PROJECT_NAME,\n",
    "        )\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name=run_name,\n",
    "            reinit=True,\n",
    "            project=PROJECT_NAME,\n",
    "            config=CONFIG\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c04d08",
   "metadata": {},
   "source": [
    "# Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadfc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 22:26:52,705 - INFO - Starting training on cuda for 10 epochs...\n",
      "2025-10-23 22:26:52,706 - INFO - \n",
      "Epoch 1/10\n",
      "Training Epoch:   0%|          | 0/250 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_losses, val_losses = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNUM_EPOCHS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(num_epochs, model, train_loader, val_loader, optimizer, scheduler, criterion)\u001b[39m\n\u001b[32m     18\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m train_losses.append(train_loss)\n\u001b[32m     28\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, criterion)\u001b[39m\n\u001b[32m     30\u001b[39m     optimizer.zero_grad()\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Check for NaN in loss\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(loss):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/Notebooks/../losses/cloobLoss.py:58\u001b[39m, in \u001b[36mCLOOBLoss.forward\u001b[39m\u001b[34m(self, image_features, text_features)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_features, text_features):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloob_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minv_tau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_hopfield\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/Notebooks/../losses/cloobLoss.py:19\u001b[39m, in \u001b[36mcloob_loss\u001b[39m\u001b[34m(image_features, text_features, inv_tau, scale_hopfield)\u001b[39m\n\u001b[32m     17\u001b[39m p_xx, p_yy, p_xy, p_yx = hopfield_retrieval(image_features, text_features, scale_hopfield)\n\u001b[32m     18\u001b[39m identity = torch.eye(p_xx.shape[\u001b[32m1\u001b[39m]) > \u001b[32m0.5\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m loss_img = \u001b[43minfoloob_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_xx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_xy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midentity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43minv_tau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m loss_txt = infoloob_loss(p_yy.T, p_yx.T, identity, inv_tau=inv_tau)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (loss_img + loss_txt) / \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/School/Senior/Multimodal-2025/Notebooks/../losses/cloobLoss.py:27\u001b[39m, in \u001b[36minfoloob_loss\u001b[39m\u001b[34m(x, y, i, inv_tau)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Computes the InfoLOOB loss (negative mean log odds assigned to positive pairs).\"\"\"\u001b[39;00m\n\u001b[32m     26\u001b[39m k = x @ y.T * inv_tau\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m positives = -torch.mean(torch.sum(\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m, dim=\u001b[32m1\u001b[39m))\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# For logsumexp the zero entries must be equal to a very large negative number\u001b[39;00m\n\u001b[32m     29\u001b[39m large_neg = -\u001b[32m10000.\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train(\n",
    "    CONFIG[\"NUM_EPOCHS\"],\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    loss # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae0e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(MODEL, train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardelephant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
