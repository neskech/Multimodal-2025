{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae254ea",
   "metadata": {},
   "source": [
    "# Metrics Notebook\n",
    "This is a simple notebook designed to run all the metrics for a given variant of CLIP.  It expects a model as defined in the config below that is \"CLIP-like\" in that it can take in an image or text and output an embedding of some size.  The clip-like model should specifically adhere to the Clip interface defined in `models/clip.py`\n",
    "\n",
    "The four metrics we implement are outlined here: \n",
    "\n",
    "### Top-K Retrieval Accuracy.\n",
    "Given an image, we compute its CLIP embedding and retrieve the closest K captions based on cosine similarity with caption embeddings. If the target caption is within the top K, we count this as a correct retrieval. This metric is a direct proxy for classification accuracy in multimodal retrieval. It is valuable because strong cross-modal alignment should yield high retrieval accuracy. However, since our approach aims to reduce sparsity on the hypersphere, the embeddings may become less linearly separable, potentially lowering retrieval performance even as uniformity improves.\n",
    "\n",
    "### Modality Gap via Linear Separability.\n",
    "Following \\citet{modalityGAP}, we measure the modality gap between text and image embeddings by training a soft-margin SVM classifier to distinguish modality type. We evaluate classification accuracy, precision, and recall. High separability indicates a strong modality gap, which is undesirable because semantically matched image–text pairs should ideally share indistinguishable representations. Reducing modality separability would thus reflect improved multimodal coordination.\n",
    "\n",
    "### Hyperspherical Entropy Estimation.\n",
    "We measure the entropy of the embedding distribution on the hypersphere using the k-nearest neighbor–based estimator proposed by \\citet{entropy}. This estimator leverages angular distances to compute local density estimates, which are aggregated into a global entropy measure. Entropy serves as a proxy for sparsity: low entropy distributions are clustered and “spiky,” while high entropy indicates more uniform coverage of the hypersphere. Since our method encourages uniformity, we expect an increase in entropy relative to standard CLIP.\n",
    "\n",
    "### Downstream Captioning Performance (BLEU)\n",
    "Finally, we evaluate the utility of embeddings on a generative downstream task: image captioning. Image embeddings are passed into a pretrained language model to generate captions, which are compared against ground-truth captions using BLEU score. BLEU measures n-gram overlap between generated and reference text, rewarding fluency and accuracy. This extrinsic metric demonstrates how improvements in embedding geometry translate into practical benefits for end-user tasks, beyond abstract geometric properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e3fa51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.clipModel import CLIPModel\n",
    "\n",
    "\n",
    "model = CLIPModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16da319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def top_k_similarities(embeddings, query_embedding, k=5):\n",
    "    \"\"\"\n",
    "    Compute the top-k most similar embeddings to the query_embedding.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): Tensor of shape (N, D) where N is the number of embeddings and D is the embedding dimension.\n",
    "        query_embedding (torch.Tensor): Tensor of shape (D,) representing the query embedding.\n",
    "        k (int): Number of top similar embeddings to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, float]]: List of tuples containing the index and similarity score of the top-k most similar embeddings.\n",
    "    \"\"\"\n",
    "    # Compute cosine similarities\n",
    "    similarities = torch.nn.functional.cosine_similarity(embeddings, query_embedding.unsqueeze(0), dim=1)\n",
    "\n",
    "    # Get top-k indices\n",
    "    top_k_indices = similarities.topk(k).indices\n",
    "\n",
    "    # Return list of (index, similarity) tuples\n",
    "    return [(idx.item(), similarities[idx].item()) for idx in top_k_indices]\n",
    "\n",
    "def top_k_score(embedding_pairs, k=5):\n",
    "    \"\"\"\n",
    "    Given a list of (text_embedding[], image_embedding) pairs, return the percentage of texts that are in the top-k most similar to their corresponding image embeddings.\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    for text_embeddings, image_embedding in embedding_pairs:\n",
    "        top_k = top_k_similarities(text_embeddings, image_embedding, k)\n",
    "        if 0 in [idx for idx, _ in top_k]:  # Assuming the correct text is always at index 0\n",
    "            correct_count += 1\n",
    "    return correct_count / len(embedding_pairs) if embedding_pairs else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b729df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def linear_separability(image_embeddings, text_embeddings, num_epochs=100, learning_rate=1e-3):\n",
    "    \"\"\"\n",
    "    Train a linear classifier to distinguish between image and text embeddings, and report the accuracy.\n",
    "    \n",
    "    Args:\n",
    "        image_embeddings (torch.Tensor): Tensor of shape (N, D) for image embeddings.\n",
    "        text_embeddings (torch.Tensor): Tensor of shape (N, D) for text embeddings.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the classifier on the given set.\n",
    "    \"\"\"\n",
    "    # Combine image and text embeddings\n",
    "    embeddings = torch.cat([image_embeddings, text_embeddings], dim=0)\n",
    "    labels = torch.cat([torch.zeros(image_embeddings.size(0)), torch.ones(text_embeddings.size(0))], dim=0)\n",
    "\n",
    "    # Train a linear classifier\n",
    "    classifier = nn.Linear(embeddings.size(1), 2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = classifier(embeddings)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    with torch.no_grad():\n",
    "        outputs = classifier(embeddings)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        accuracy = (preds == labels).float().mean().item()\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be655bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu_score(predictions, references):\n",
    "    \"\"\"\n",
    "    Compute a simple BLEU score for a list of predictions and references.\n",
    "    \n",
    "    Args:\n",
    "        predictions (List[str]): List of predicted sentences.\n",
    "        references (List[str]): List of reference sentences.\n",
    "\n",
    "    Returns:\n",
    "        float: Average BLEU score across all predictions.\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "    total_score = 0.0\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        ref_tokens = [ref.split()]\n",
    "        pred_tokens = pred.split()\n",
    "        score = sentence_bleu(ref_tokens, pred_tokens)\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(predictions) if predictions else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f068d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypersphere_cap_area(phi, d):\n",
    "        \"\"\"\n",
    "        Compute the area of a spherical cap with angle phi on a (d-1)-sphere.\n",
    "        \n",
    "        S(φ) = (1/2) * S_p * [1 - sgn(cos φ) * I_{cos²φ}(1/2, (p-1)/2)]\n",
    "        \n",
    "        where S_p is the surface area of the (d-1)-sphere and I is the regularized \n",
    "        incomplete beta function.\n",
    "        \"\"\"\n",
    "        # Surface area of (d-1)-sphere: S_p = 2π^(d/2) / Γ(d/2)\n",
    "        from scipy.special import gamma, betainc\n",
    "        S_p = 2 * (np.pi ** (d/2)) / gamma(d/2)\n",
    "        print(S_p)  # --- IGNORE ---\n",
    "        \n",
    "        cos_phi = np.cos(phi)\n",
    "        cos_phi_squared = cos_phi ** 2\n",
    "        cos_phi_sgn = np.sign(cos_phi)\n",
    "\n",
    "        alpha = 0.5\n",
    "        beta = (d - 1) / 2\n",
    "        incomplete_beta = betainc(alpha, beta, cos_phi_squared)\n",
    "        print((1 - cos_phi_sgn * incomplete_beta))\n",
    "        cap_area = 0.5 * S_p * (1 - cos_phi_sgn * incomplete_beta)\n",
    "        assert cap_area > 0, f\"Cap area must be positive, not {cap_area}.\"\n",
    "        return cap_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4bc62d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.628037009147083e-56)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import gamma, betainc\n",
    "\n",
    "d = 128\n",
    "2 * (np.pi ** (d/2)) / gamma(d/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c007aafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.566370614359174\n",
      "0.0049958347219741794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.03138975532220578)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypersphere_cap_area(.1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f273c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a515b3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import digamma, beta\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def knn_entropy_notgpt(embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Compute the k-nearest neighbor entropy estimator for hyperspherical data.\n",
    "    \n",
    "    This estimator is designed for data on a unit hypersphere and uses the \n",
    "    k-nearest neighbor approach to estimate entropy consistently.\n",
    "    \n",
    "    Args:\n",
    "        embeddings (torch.Tensor): Tensor of shape (N, D) representing N embeddings on the unit hypersphere of dimension D-1.\n",
    "        k (int): Number of nearest neighbors to consider.\n",
    "    \n",
    "    Returns:\n",
    "        float: Estimated entropy of the distribution.\n",
    "    \"\"\"\n",
    "    # Ensure embeddings are normalized to unit sphere\n",
    "    embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    n, d = embeddings_np.shape\n",
    "    \n",
    "    # Compute pairwise angular distances using arccos(x^T y)\n",
    "    # Since embeddings are normalized, dot product gives cosine similarity\n",
    "    dot_products = np.dot(embeddings_np, embeddings_np.T)\n",
    "    # Clamp to avoid numerical issues with arccos\n",
    "    dot_products = np.clip(dot_products, -1.0, 1.0)\n",
    "    angular_distances = np.arccos(dot_products)\n",
    "    \n",
    "    # For each point, find the k-th nearest neighbor distance\n",
    "    phi_values = []\n",
    "    for i in range(n):\n",
    "        # Get distances to all other points (excluding self)\n",
    "        distances_to_i = angular_distances[i]\n",
    "        distances_to_i = np.delete(distances_to_i, i)  # Remove self-distance (which is 0)\n",
    "        \n",
    "        # Sort and get k-th nearest neighbor distance\n",
    "        sorted_distances = np.sort(distances_to_i)\n",
    "        phi_i = sorted_distances[k-1]  # k-th nearest (0-indexed)\n",
    "        phi_values.append(phi_i)\n",
    "\n",
    "    phi_values = np.array(phi_values)\n",
    "    print(phi_values)\n",
    "\n",
    "    # Compute cap areas for all phi values\n",
    "    S_phi = np.array([hypersphere_cap_area(phi, d) for phi in phi_values])\n",
    "    # Compute L_{n,i} = ln(f_n(X_i)) = ln(k/n / S(phi_i))\n",
    "    L_values = np.log(k/n) - np.log(S_phi)\n",
    "    \n",
    "    # Compute digamma function ψ(k)\n",
    "    psi_k = digamma(k)\n",
    "    \n",
    "    # Compute entropy using the first formulation:\n",
    "    # H_n(f) = -(1/n) * Σ[L_{n,i} - ln(k) + ψ(k)]\n",
    "    entropy = -(1/n) * np.sum(L_values - np.log(k) + psi_k)\n",
    "    \n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759f62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import digamma, gamma\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def knn_entropy(X, k=4):\n",
    "    print(\"X shape: \", X.shape)\n",
    "    if isinstance(X, torch.Tensor):\n",
    "        X = X.detach().cpu().numpy()\n",
    "    X /= np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    # X: (N, D) array, rows are unit vectors\n",
    "    N, D = X.shape\n",
    "    d = D - 1  # intrinsic dimension of S^{D-1}\n",
    "\n",
    "    # compute cosine similarities and clip for numerical safety\n",
    "    sims = X.dot(X.T)\n",
    "    np.fill_diagonal(sims, 1.0)\n",
    "    sims = np.clip(sims, -1.0, 1.0)\n",
    "\n",
    "    # geodesic distances (great-circle)\n",
    "    ang = np.arccos(sims)   # matrix of angles in [0, pi]\n",
    "    print(\"hello\")\n",
    "    # use NearestNeighbors on precomputed distances\n",
    "    nbrs = NearestNeighbors(n_neighbors=k+1, metric='precomputed')\n",
    "    nbrs.fit(ang)  # note: fit expects (N, N) precomputed distance matrix\n",
    "    print(\"hello2\")\n",
    "    distances, indices = nbrs.kneighbors(ang)\n",
    "    # distances[:,0] is zero (self); k-th neighbor is distances[:, k]\n",
    "    r_k = distances[:, k]   # geodesic radius to k-th neighbor\n",
    "\n",
    "    eps = 2.0 * r_k  # diameter as in KL notation\n",
    "\n",
    "    print(\"hello3\")\n",
    "    # volume constant c_d for Euclidean d-ball but note factor 2^d in some conventions\n",
    "    c_d = (np.pi**(d/2)) / (gamma(1 + d/2) * (2.0**d))\n",
    "\n",
    "    H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
    "    print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
    "    return H_hat  # in nats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2450695c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (300, 128)\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.5061176684318003 5.702114882064637 -218.42614901550428 128.84635272811022\n",
      "Estimated entropy: -85.38379907376122\n"
     ]
    }
   ],
   "source": [
    "# Test knn entropy\n",
    "X_test = np.random.randn(300, 128)\n",
    " # normalize to unit sphere\n",
    "entropy_estimate = knn_entropy(X_test, k=5)\n",
    "print(\"Estimated entropy:\", entropy_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb240db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative implementation using the second formulation for verification\n",
    "def knn_entropy_alternative(embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Alternative implementation using the second formulation:\n",
    "    H_n(f) = (1/n) * Σ ln[n * S(φ_i)] - ψ(k)\n",
    "    \"\"\"\n",
    "    # Ensure embeddings are normalized to unit sphere\n",
    "    embeddings = embeddings / torch.norm(embeddings, dim=1, keepdim=True)\n",
    "    embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    \n",
    "    n, d = embeddings_np.shape\n",
    "    \n",
    "    # Compute pairwise angular distances\n",
    "    dot_products = np.dot(embeddings_np, embeddings_np.T)\n",
    "    dot_products = np.clip(dot_products, -1.0, 1.0)\n",
    "    angular_distances = np.arccos(dot_products)\n",
    "    \n",
    "    # Find k-th nearest neighbor distances\n",
    "    phi_values = []\n",
    "    for i in range(n):\n",
    "        distances_to_i = angular_distances[i]\n",
    "        distances_to_i = np.delete(distances_to_i, i)\n",
    "        sorted_distances = np.sort(distances_to_i)\n",
    "        phi_i = sorted_distances[k-1]\n",
    "        phi_values.append(phi_i)\n",
    "    \n",
    "    phi_values = np.array(phi_values)\n",
    "    \n",
    "    # Compute cap areas using the same simplified geometric formula\n",
    "    def hypersphere_cap_area(phi, d):\n",
    "        \"\"\"\n",
    "        Compute the area of a spherical cap with angular radius phi on a unit (d-1)-sphere.\n",
    "        This is the same simplified implementation as in the main function.\n",
    "        \"\"\"\n",
    "        from scipy.special import gamma\n",
    "        \n",
    "        # Surface area of the full (d-1)-sphere: S_d = 2π^(d/2) / Γ(d/2)\n",
    "        S_full = 2 * (np.pi ** (d/2)) / gamma(d/2)\n",
    "        \n",
    "        # For small angles, use series expansion to avoid numerical issues\n",
    "        if phi < 1e-10:\n",
    "            cap_area = (np.pi ** ((d-1)/2) / gamma((d+1)/2)) * (phi ** (d-1))\n",
    "        elif phi >= np.pi:\n",
    "            cap_area = S_full\n",
    "        else:\n",
    "            if d == 2:\n",
    "                # Special case for 2D (circle): cap area = 2 * phi\n",
    "                cap_area = 2 * phi\n",
    "            elif d == 3:\n",
    "                # Special case for 3D (sphere): cap area = 2π * (1 - cos(phi))\n",
    "                cap_area = 2 * np.pi * (1 - np.cos(phi))\n",
    "            else:\n",
    "                # General case: use numerical integration\n",
    "                from scipy.integrate import quad\n",
    "                \n",
    "                def integrand(theta):\n",
    "                    return np.sin(theta) ** (d - 2)\n",
    "                \n",
    "                numerator, _ = quad(integrand, 0, phi)\n",
    "                denominator, _ = quad(integrand, 0, np.pi)\n",
    "                cap_area = S_full * (numerator / denominator)\n",
    "        \n",
    "        return max(cap_area, 1e-15)\n",
    "    \n",
    "    S_phi = np.array([hypersphere_cap_area(phi, d) for phi in phi_values])\n",
    "    \n",
    "    # Second formulation: H_n(f) = (1/n) * Σ ln[n * S(φ_i)] - ψ(k)\n",
    "    psi_k = digamma(k)\n",
    "    entropy = (1/n) * np.sum(np.log(n * S_phi)) - psi_k\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04be2e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 18:31:28,838 - INFO - Loading COCO val2017 dataset...\n",
      "2025-10-20 18:31:28,902 - INFO - Loaded 500 COCO samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Hyperspherical Entropy Estimator on COCO Data\n",
      "============================================================\n",
      "Loading COCO dataset...\n",
      "Loaded 500 COCO samples\n",
      "\n",
      "Generating CLIP embeddings...\n",
      "Generating image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.78it/s]\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:36: RuntimeWarning: divide by zero encountered in log\n",
      "  H_hat = -digamma(k) + digamma(N) + np.log(c_d) + (d / N) * np.sum(np.log(eps + 1e-12))\n",
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49953/1467022755.py:37: RuntimeWarning: divide by zero encountered in log\n",
      "  print(-digamma(k), digamma(N), np.log(c_d), (d / N) * np.sum(np.log(eps + 1e-12)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Image embeddings shape: torch.Size([100, 512])\n",
      "Text embeddings shape: torch.Size([100, 512])\n",
      "\n",
      "Computing entropy estimates...\n",
      "\n",
      "Testing with k=3:\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-0.9227843350984671 4.600161852738088 -inf -5803.1406\n",
      "  Image entropy (method 1): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-0.9227843350984671 4.600161852738088 -inf 149.63445\n",
      "  Text entropy (method 1): -inf\n",
      "\n",
      "Testing with k=5:\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.5061176684318003 4.600161852738088 -inf 241.86328\n",
      "  Image entropy (method 1): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.5061176684318003 4.600161852738088 -inf 223.04105\n",
      "  Text entropy (method 1): -inf\n",
      "\n",
      "Testing with k=7:\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.872784335098467 4.600161852738088 -inf 261.00137\n",
      "  Image entropy (method 1): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.872784335098467 4.600161852738088 -inf 248.91978\n",
      "  Text entropy (method 1): -inf\n",
      "\n",
      "Testing with k=10:\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-2.251752589066721 4.600161852738088 -inf 290.65335\n",
      "  Image entropy (method 1): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-2.251752589066721 4.600161852738088 -inf 282.28818\n",
      "  Text entropy (method 1): -inf\n",
      "\n",
      "Testing with k=15:\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-2.6743466616607936 4.600161852738088 -inf 314.78067\n",
      "  Image entropy (method 1): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-2.6743466616607936 4.600161852738088 -inf 303.85056\n",
      "  Text entropy (method 1): -inf\n",
      "\n",
      "Generating uniform random baseline...\n",
      "Computing entropy for uniform random embeddings...\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-0.9227843350984671 4.600161852738088 -inf 556.11694\n",
      "  Random entropy (k=3): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.5061176684318003 4.600161852738088 -inf 560.03955\n",
      "  Random entropy (k=5): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-1.872784335098467 4.600161852738088 -inf 562.9639\n",
      "  Random entropy (k=7): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-2.251752589066721 4.600161852738088 -inf 566.17615\n",
      "  Random entropy (k=10): -inf\n",
      "X shape:  torch.Size([100, 512])\n",
      "hello\n",
      "hello2\n",
      "hello3\n",
      "-2.6743466616607936 4.600161852738088 -inf 570.1874\n",
      "  Random entropy (k=15): -inf\n"
     ]
    }
   ],
   "source": [
    "# Test the hyperspherical entropy estimator on COCO data\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasetLoader import DatasetLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_entropy_estimator_on_coco():\n",
    "    \"\"\"\n",
    "    Test the hyperspherical entropy estimator on COCO embeddings.\n",
    "    This will:\n",
    "    1. Load COCO dataset samples\n",
    "    2. Generate CLIP embeddings for images and text\n",
    "    3. Compute entropy for both modalities\n",
    "    4. Compare with random uniform embeddings as a baseline\n",
    "    \"\"\"\n",
    "    print(\"Testing Hyperspherical Entropy Estimator on COCO Data\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load COCO dataset samples\n",
    "    try:\n",
    "        print(\"Loading COCO dataset...\")\n",
    "        data_samples = DatasetLoader.load_coco_dataset(\n",
    "            data_dir=\"data\",\n",
    "            split=\"val2017\",\n",
    "            max_samples=500  # Use smaller sample for testing\n",
    "        )\n",
    "        print(f\"Loaded {len(data_samples)} COCO samples\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load COCO dataset: {e}\")\n",
    "        print(\"Generating synthetic data for testing...\")\n",
    "        # Create synthetic data for testing\n",
    "        data_samples = []\n",
    "        for i in range(100):\n",
    "            data_samples.append({\n",
    "                \"image_path\": f\"dummy_image_{i}.jpg\",\n",
    "                \"text\": f\"A sample caption number {i}\"\n",
    "            })\n",
    "    \n",
    "    # Generate embeddings using the CLIP model\n",
    "    print(\"\\nGenerating CLIP embeddings...\")\n",
    "    \n",
    "    # For testing, we'll use a subset to make it faster\n",
    "    test_samples = data_samples[:100]\n",
    "    \n",
    "    try:\n",
    "        # Generate image embeddings\n",
    "        print(\"Generating image embeddings...\")\n",
    "        image_paths = [sample[\"image_path\"] for sample in test_samples]\n",
    "        image_embeddings = []\n",
    "        \n",
    "        # Process in smaller batches to avoid memory issues\n",
    "        batch_size = 10\n",
    "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            try:\n",
    "                batch_embeddings = model.encode_images(batch_paths)\n",
    "                image_embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding batch {i//batch_size}: {e}\")\n",
    "                # Generate random embeddings as fallback\n",
    "                for _ in batch_paths:\n",
    "                    random_emb = torch.randn(512)  # Assume 512-dim embeddings\n",
    "                    random_emb = random_emb / torch.norm(random_emb)  # Normalize to unit sphere\n",
    "                    image_embeddings.append(random_emb)\n",
    "        \n",
    "        image_embeddings = torch.stack(image_embeddings)\n",
    "        \n",
    "        # Generate text embeddings\n",
    "        print(\"Generating text embeddings...\")\n",
    "        texts = [sample[\"text\"] for sample in test_samples]\n",
    "        text_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch_texts = texts[i:i+batch_size]\n",
    "            try:\n",
    "                batch_embeddings = model.encode_text(batch_texts)\n",
    "                text_embeddings.extend(batch_embeddings)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding text batch {i//batch_size}: {e}\")\n",
    "                # Generate random embeddings as fallback\n",
    "                for _ in batch_texts:\n",
    "                    random_emb = torch.randn(512)\n",
    "                    random_emb = random_emb / torch.norm(random_emb)\n",
    "                    text_embeddings.append(random_emb)\n",
    "        \n",
    "        text_embeddings = torch.stack(text_embeddings)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embeddings: {e}\")\n",
    "        print(\"Using synthetic embeddings for testing...\")\n",
    "        \n",
    "        # Generate synthetic embeddings for testing\n",
    "        embedding_dim = 512\n",
    "        num_samples = len(test_samples)\n",
    "        \n",
    "        # Generate clustered embeddings (lower entropy)\n",
    "        image_embeddings = torch.randn(num_samples, embedding_dim)\n",
    "        image_embeddings = image_embeddings / torch.norm(image_embeddings, dim=1, keepdim=True)\n",
    "        \n",
    "        text_embeddings = torch.randn(num_samples, embedding_dim)\n",
    "        text_embeddings = text_embeddings / torch.norm(text_embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    print(f\"\\nImage embeddings shape: {image_embeddings.shape}\")\n",
    "    print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "    \n",
    "    # Test entropy estimation with different k values\n",
    "    k_values = [3, 5, 7, 10, 15]\n",
    "    \n",
    "    print(\"\\nComputing entropy estimates...\")\n",
    "    results = {\n",
    "        'k_values': k_values,\n",
    "        'image_entropy': [],\n",
    "        'text_entropy': [],\n",
    "        'image_entropy_alt': [],\n",
    "        'text_entropy_alt': []\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        print(f\"\\nTesting with k={k}:\")\n",
    "        \n",
    "        # Compute entropy for image embeddings\n",
    "        try:\n",
    "            img_entropy = knn_entropy(image_embeddings, k=k)\n",
    "            print(f\"  Image entropy (method 1): {img_entropy:.4f}\")\n",
    "            \n",
    "            results['image_entropy'].append(img_entropy)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error computing image entropy: {e}\")\n",
    "            results['image_entropy'].append(np.nan)\n",
    "            results['image_entropy_alt'].append(np.nan)\n",
    "        \n",
    "        # Compute entropy for text embeddings\n",
    "        try:\n",
    "            txt_entropy = knn_entropy(text_embeddings, k=k)\n",
    "            print(f\"  Text entropy (method 1): {txt_entropy:.4f}\")\n",
    "            \n",
    "            results['text_entropy'].append(txt_entropy)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error computing text entropy: {e}\")\n",
    "            results['text_entropy'].append(np.nan)\n",
    "            results['text_entropy_alt'].append(np.nan)\n",
    "    \n",
    "    # Generate baseline: uniform random embeddings on hypersphere\n",
    "    print(\"\\nGenerating uniform random baseline...\")\n",
    "    embedding_dim = image_embeddings.shape[1]\n",
    "    num_samples = image_embeddings.shape[0]\n",
    "    \n",
    "    # Generate uniform random embeddings on hypersphere\n",
    "    random_embeddings = torch.randn(num_samples, embedding_dim)\n",
    "    random_embeddings = random_embeddings / torch.norm(random_embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    print(\"Computing entropy for uniform random embeddings...\")\n",
    "    random_entropies = []\n",
    "    for k in k_values:\n",
    "        try:\n",
    "            random_entropy = knn_entropy(random_embeddings, k=k)\n",
    "            random_entropies.append(random_entropy)\n",
    "            print(f\"  Random entropy (k={k}): {random_entropy:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error computing random entropy (k={k}): {e}\")\n",
    "            random_entropies.append(np.nan)\n",
    "    \n",
    "    results['random_entropy'] = random_entropies\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Entropy vs k for different data types\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(k_values, results['image_entropy'], 'bo-', label='Image Embeddings', markersize=6)\n",
    "    plt.plot(k_values, results['text_entropy'], 'ro-', label='Text Embeddings', markersize=6)\n",
    "    plt.plot(k_values, results['random_entropy'], 'go-', label='Random Uniform', markersize=6)\n",
    "    plt.xlabel('k (number of neighbors)')\n",
    "    plt.ylabel('Entropy Estimate')\n",
    "    plt.title('Hyperspherical Entropy vs k')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Comparison of two estimation methods\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(k_values, results['image_entropy'], 'b-', label='Image (Method 1)', linewidth=2)\n",
    "    plt.plot(k_values, results['text_entropy'], 'r-', label='Text (Method 1)', linewidth=2)\n",
    "    plt.xlabel('k (number of neighbors)')\n",
    "    plt.ylabel('Entropy Estimate')\n",
    "    plt.title('Comparison of Estimation Methods')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Histogram of embedding norms (should be close to 1)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    img_norms = torch.norm(image_embeddings, dim=1).detach().numpy()\n",
    "    txt_norms = torch.norm(text_embeddings, dim=1).detach().numpy()\n",
    "    plt.hist(img_norms, bins=20, alpha=0.6, label='Image Embeddings', density=True)\n",
    "    plt.hist(txt_norms, bins=20, alpha=0.6, label='Text Embeddings', density=True)\n",
    "    plt.axvline(x=1.0, color='red', linestyle='--', label='Unit Norm')\n",
    "    plt.xlabel('Embedding Norm')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Distribution of Embedding Norms')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ENTROPY ESTIMATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Average entropy across k values (excluding NaN)\n",
    "    img_avg = np.nanmean(results['image_entropy'])\n",
    "    txt_avg = np.nanmean(results['text_entropy'])\n",
    "    rand_avg = np.nanmean(results['random_entropy'])\n",
    "    \n",
    "    print(f\"Average Entropy Estimates:\")\n",
    "    print(f\"  Image Embeddings: {img_avg:.4f}\")\n",
    "    print(f\"  Text Embeddings:  {txt_avg:.4f}\")\n",
    "    print(f\"  Random Uniform:   {rand_avg:.4f}\")\n",
    "    \n",
    "    # Check if embeddings are properly normalized\n",
    "    img_norm_mean = torch.norm(image_embeddings, dim=1).mean().item()\n",
    "    txt_norm_mean = torch.norm(text_embeddings, dim=1).mean().item()\n",
    "    print(f\"\\nEmbedding Normalization Check:\")\n",
    "    print(f\"  Image embeddings mean norm: {img_norm_mean:.6f}\")\n",
    "    print(f\"  Text embeddings mean norm:  {txt_norm_mean:.6f}\")\n",
    "    print(f\"  (Should be close to 1.0 for unit hypersphere)\")\n",
    "    \n",
    "    # Interpretation\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if rand_avg > max(img_avg, txt_avg):\n",
    "        print(\"✓ Random uniform embeddings have higher entropy than CLIP embeddings\")\n",
    "        print(\"  This suggests CLIP embeddings are more clustered/structured\")\n",
    "    else:\n",
    "        print(\"! CLIP embeddings have higher entropy than uniform random\")\n",
    "        print(\"  This might indicate issues with the estimation or data\")\n",
    "    \n",
    "    if abs(img_avg - txt_avg) > 0.5:\n",
    "        modality_gap = \"large\"\n",
    "    elif abs(img_avg - txt_avg) > 0.2:\n",
    "        modality_gap = \"moderate\"\n",
    "    else:\n",
    "        modality_gap = \"small\"\n",
    "    \n",
    "    print(f\"  Modality gap in entropy: {modality_gap} ({abs(img_avg - txt_avg):.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the test\n",
    "test_results = test_entropy_estimator_on_coco()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3919a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating Entropy Estimator with Synthetic Data\n",
      "==================================================\n",
      "\n",
      "Test 1: Uniform Random Distribution\n",
      "X shape:  torch.Size([300, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/xyqlxcts5bv_7ymh8fvwn8q40000gn/T/ipykernel_49492/3238757963.py:7: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  X /= np.linalg.norm(X, axis=1, keepdims=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "1D tensors expected, but got 2D and 2D tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    123\u001b[39m         \u001b[33m'\u001b[39m\u001b[33muniform_entropy\u001b[39m\u001b[33m'\u001b[39m: (uniform_entropy_1),\n\u001b[32m    124\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mclustered_entropy\u001b[39m\u001b[33m'\u001b[39m: (clustered_entropy_1),\n\u001b[32m    125\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconcentrated_entropy\u001b[39m\u001b[33m'\u001b[39m: (concentrated_entropy_1),\n\u001b[32m    126\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mvalidation_passed\u001b[39m\u001b[33m'\u001b[39m: entropy_ordering_correct\n\u001b[32m    127\u001b[39m     }\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Run validation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m validation_results = \u001b[43mvalidate_entropy_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mvalidate_entropy_estimator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m uniform_embeddings = torch.randn(num_samples, embedding_dim)\n\u001b[32m     16\u001b[39m uniform_embeddings = uniform_embeddings / torch.norm(uniform_embeddings, dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m uniform_entropy_1 = \u001b[43mknn_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniform_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Uniform entropy (method 1): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muniform_entropy_1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Test 2: Clustered distribution (should have lower entropy)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mknn_entropy\u001b[39m\u001b[34m(X, k)\u001b[39m\n\u001b[32m     10\u001b[39m d = D - \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# intrinsic dimension of S^{D-1}\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# compute cosine similarities and clip for numerical safety\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m sims = \u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m np.fill_diagonal(sims, \u001b[32m1.0\u001b[39m)\n\u001b[32m     15\u001b[39m sims = np.clip(sims, -\u001b[32m1.0\u001b[39m, \u001b[32m1.0\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: 1D tensors expected, but got 2D and 2D tensors"
     ]
    }
   ],
   "source": [
    "def validate_entropy_estimator():\n",
    "    \"\"\"\n",
    "    Validate the entropy estimator with synthetic data where we can control the distribution properties.\n",
    "    \"\"\"\n",
    "    print(\"Validating Entropy Estimator with Synthetic Data\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test parameters\n",
    "    embedding_dim = 128\n",
    "    num_samples = 300\n",
    "    k = 30\n",
    "    \n",
    "    # Test 1: Uniform random distribution on hypersphere (should have high entropy)\n",
    "    print(\"\\nTest 1: Uniform Random Distribution\")\n",
    "    uniform_embeddings = torch.randn(num_samples, embedding_dim)\n",
    "    uniform_embeddings = uniform_embeddings / torch.norm(uniform_embeddings, dim=1, keepdim=True)\n",
    "\n",
    "    uniform_entropy_1 = knn_entropy(uniform_embeddings, k=k)\n",
    "    \n",
    "    print(f\"  Uniform entropy (method 1): {uniform_entropy_1:.4f}\")\n",
    "    \n",
    "    # Test 2: Clustered distribution (should have lower entropy)\n",
    "    print(\"\\nTest 2: Clustered Distribution (3 clusters)\")\n",
    "    clustered_embeddings = []\n",
    "    samples_per_cluster = num_samples // 3\n",
    "    \n",
    "    # Create 3 clusters\n",
    "    cluster_centers = torch.randn(3, embedding_dim)\n",
    "    cluster_centers = cluster_centers / torch.norm(cluster_centers, dim=1, keepdim=True)\n",
    "    \n",
    "    for i in range(3):\n",
    "        # Generate points around each cluster center\n",
    "        cluster_points = cluster_centers[i].unsqueeze(0) + 0.01 * torch.randn(samples_per_cluster, embedding_dim)\n",
    "        cluster_points = cluster_points / torch.norm(cluster_points, dim=1, keepdim=True)\n",
    "        clustered_embeddings.append(cluster_points)\n",
    "    \n",
    "    clustered_embeddings = torch.cat(clustered_embeddings, dim=0)\n",
    "    clustered_entropy_1 = knn_entropy(clustered_embeddings, k=k)\n",
    "    \n",
    "    print(f\"  Clustered entropy (method 1): {clustered_entropy_1:.4f}\")\n",
    "    \n",
    "    # Test 3: Very concentrated distribution (should have very low entropy)\n",
    "    print(\"\\nTest 3: Highly Concentrated Distribution\")\n",
    "    concentrated_embeddings = torch.randn(1, embedding_dim) + 0.01 * torch.randn(num_samples, embedding_dim)  # Small variance\n",
    "    concentrated_embeddings = concentrated_embeddings / torch.norm(concentrated_embeddings, dim=1, keepdim=True)\n",
    "    \n",
    "    concentrated_entropy_1 = knn_entropy(concentrated_embeddings, k=k)\n",
    "    \n",
    "    print(f\"  Concentrated entropy (method 1): {concentrated_entropy_1:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot entropy comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    distributions = ['Uniform', 'Clustered', 'Concentrated']\n",
    "    entropies_1 = [uniform_entropy_1, clustered_entropy_1, concentrated_entropy_1]\n",
    "    \n",
    "    x = np.arange(len(distributions))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, entropies_1, width, label='Method 1', alpha=0.8)\n",
    "    plt.xlabel('Distribution Type')\n",
    "    plt.ylabel('Entropy Estimate')\n",
    "    plt.title('Entropy Estimates for Different Distributions')\n",
    "    plt.xticks(x, distributions)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2D projections to visualize distributions (for first 2 dimensions)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(uniform_embeddings[:, 0], uniform_embeddings[:, 1], alpha=0.6, s=20, label='Uniform')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Uniform Distribution (2D Projection)')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    for i in range(3):\n",
    "        start_idx = i * samples_per_cluster\n",
    "        end_idx = (i + 1) * samples_per_cluster\n",
    "        plt.scatter(clustered_embeddings[start_idx:end_idx, 0], \n",
    "                   clustered_embeddings[start_idx:end_idx, 1], \n",
    "                   alpha=0.6, s=20, c=colors[i], label=f'Cluster {i+1}')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Clustered Distribution (2D Projection)')\n",
    "    plt.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(concentrated_embeddings[:, 0], concentrated_embeddings[:, 1], alpha=0.6, s=20, color='purple')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.title('Concentrated Distribution (2D Projection)')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary and validation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"VALIDATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check if entropy ordering makes sense\n",
    "    entropy_ordering_correct = uniform_entropy_1 > clustered_entropy_1 > concentrated_entropy_1\n",
    "    \n",
    "    print(f\"Expected entropy ordering (Uniform > Clustered > Concentrated): {'✓' if entropy_ordering_correct else '✗'}\")\n",
    "    \n",
    "    if entropy_ordering_correct:\n",
    "        print(\"✓ Entropy estimator validation PASSED\")\n",
    "        print(\"  The estimator correctly ranks distributions by their expected entropy levels\")\n",
    "    else:\n",
    "        print(\"✗ Entropy estimator validation FAILED\")\n",
    "        print(\"  Check implementation or increase sample size\")\n",
    "    \n",
    "    return {\n",
    "        'uniform_entropy': (uniform_entropy_1),\n",
    "        'clustered_entropy': (clustered_entropy_1),\n",
    "        'concentrated_entropy': (concentrated_entropy_1),\n",
    "        'validation_passed': entropy_ordering_correct\n",
    "    }\n",
    "\n",
    "# Run validation\n",
    "validation_results = validate_entropy_estimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd395e4",
   "metadata": {},
   "source": [
    "## CLIP Captioning Model Training\n",
    "\n",
    "The code above provides a complete training pipeline for CLIP-based image captioning using GPT-2. Here's how to use it:\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **CaptionTrainingDataset**: Custom dataset that uses any CLIP model adhering to the `ClipModel` interface to generate image embeddings paired with captions.\n",
    "\n",
    "2. **train_clip_captioning_model()**: Main training function that:\n",
    "   - Sets up the ClipCaptionModel with proper dimensions based on the CLIP model\n",
    "   - Trains using cross-entropy loss on next-token prediction\n",
    "   - Evaluates using BLEU score on generated captions\n",
    "   - Saves checkpoints during training\n",
    "\n",
    "3. **evaluate_captioning_model()**: Evaluation function that computes both loss and BLEU score\n",
    "\n",
    "4. **generate_caption()**: Caption generation function for inference\n",
    "\n",
    "### Usage Example:\n",
    "\n",
    "```python\n",
    "# Train a captioning model using the loaded CLIP model\n",
    "trained_model, training_losses = train_caption_model_on_coco(\n",
    "    clip_model=model,  # Your CLIP model from above\n",
    "    data_dir=\"data\",   # Directory containing COCO dataset\n",
    "    max_samples=1000,  # Number of samples to use\n",
    "    batch_size=8,      # Adjust based on GPU memory\n",
    "    num_epochs=5       # Number of training epochs\n",
    ")\n",
    "\n",
    "# Generate a caption for a new image\n",
    "image_embedding = model.encode_images([\"path/to/image.jpg\"])\n",
    "caption = generate_caption(trained_model, image_embedding, tokenizer, device=\"cuda\")\n",
    "print(f\"Generated caption: {caption}\")\n",
    "```\n",
    "\n",
    "### BLEU Score Evaluation:\n",
    "\n",
    "The training pipeline automatically evaluates the model using BLEU score, which measures n-gram overlap between generated and reference captions. This provides a quantitative measure of caption quality that complements the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7cae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.clipModel import CLIPModel\n",
    "\n",
    "model = CLIPModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee436e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60675f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for captioning model training\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import json\n",
    "from datasetLoader import DatasetLoader\n",
    "from models.clipCaptionModel import ClipCaptionModel\n",
    "from typing import List, Tuple, Optional\n",
    "import numpy as np\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class CaptionTrainingDataset(Dataset):\n",
    "    \"\"\"Dataset for training CLIP captioning model.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_samples, clip_model, tokenizer, max_length=77):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_samples: List of data samples from DatasetLoader\n",
    "            clip_model: CLIP model for generating image embeddings\n",
    "            tokenizer: GPT-2 tokenizer\n",
    "            max_length: Maximum sequence length for tokenization\n",
    "        \"\"\"\n",
    "        self.data_samples = data_samples\n",
    "        self.clip_model = clip_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Pre-compute image embeddings to avoid recomputing during training\n",
    "        self.image_embeddings = self._precompute_image_embeddings()\n",
    "        \n",
    "    def _precompute_image_embeddings(self):\n",
    "        \"\"\"Pre-compute image embeddings for all samples.\"\"\"\n",
    "        print(\"Pre-computing image embeddings...\")\n",
    "        image_paths = [sample[\"image_path\"] for sample in self.data_samples]\n",
    "        \n",
    "        # Generate embeddings in batches to save memory\n",
    "        embeddings = []\n",
    "        batch_size = 32\n",
    "        \n",
    "        for i in tqdm(range(0, len(image_paths), batch_size)):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_embeddings = self.clip_model.encode_images(batch_paths)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return torch.stack(embeddings)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data_samples[idx]\n",
    "        image_embedding = self.image_embeddings[idx]\n",
    "        caption = sample[\"text\"]\n",
    "        \n",
    "        # Tokenize caption\n",
    "        tokens = self.tokenizer.encode(caption, max_length=self.max_length, \n",
    "                                     truncation=True, padding='max_length')\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'image_embedding': image_embedding,\n",
    "            'tokens': tokens,\n",
    "            'caption': caption\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bebe822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_caption_model(clip_model, data_samples, num_epochs=5, batch_size=8, \n",
    "                       learning_rate=2e-5, save_path=\"clip_caption_model.pt\"):\n",
    "    \"\"\"\n",
    "    Train a CLIP captioning model using the provided CLIP model and dataset.\n",
    "    \n",
    "    Args:\n",
    "        clip_model: Pre-trained CLIP model for generating image embeddings\n",
    "        data_samples: List of data samples with image paths and captions\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Training batch size\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        save_path: Path to save the trained model\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained_model, training_losses, bleu_scores)\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = CaptionTrainingDataset(data_samples, clip_model, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize captioning model\n",
    "    # Determine CLIP embedding size from the model\n",
    "    sample_embedding = clip_model.encode_images([data_samples[0][\"image_path\"]])[0]\n",
    "    clip_embed_size = sample_embedding.shape[0]\n",
    "    \n",
    "    caption_model = ClipCaptionModel(\n",
    "        prefix_length=10,\n",
    "        clip_length=10, \n",
    "        prefix_size=clip_embed_size\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(caption_model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=0, num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    training_losses = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "    caption_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            image_embeddings = batch['image_embedding'].to(device)\n",
    "            tokens = batch['tokens'].to(device)\n",
    "            \n",
    "            # Prepare input tokens (exclude last token) and labels (exclude first token)\n",
    "            input_tokens = tokens[:, :-1]\n",
    "            labels = tokens[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = caption_model(input_tokens, image_embeddings, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        training_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        # Evaluate with BLEU score every epoch\n",
    "        bleu_score = evaluate_captioning_model(caption_model, clip_model, data_samples[:50], tokenizer)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss = {avg_epoch_loss:.4f}, BLEU = {bleu_score:.4f}\")\n",
    "    \n",
    "    # Save the trained model\n",
    "    torch.save(caption_model.state_dict(), save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "    \n",
    "    return caption_model, training_losses, bleu_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2159124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_captioning_model(caption_model, clip_model, data_samples, tokenizer, max_samples=50):\n",
    "    \"\"\"\n",
    "    Evaluate the captioning model using BLEU score.\n",
    "    \n",
    "    Args:\n",
    "        caption_model: Trained captioning model\n",
    "        clip_model: CLIP model for generating image embeddings\n",
    "        data_samples: List of data samples for evaluation\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        max_samples: Maximum number of samples to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Average BLEU score\n",
    "    \"\"\"\n",
    "    caption_model.eval()\n",
    "    \n",
    "    # Limit samples for faster evaluation\n",
    "    eval_samples = data_samples[:max_samples]\n",
    "    \n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(eval_samples, desc=\"Evaluating\"):\n",
    "            # Generate image embedding\n",
    "            image_embedding = clip_model.encode_images([sample[\"image_path\"]])[0].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Generate caption\n",
    "            generated_caption = generate_caption(caption_model, image_embedding, tokenizer)\n",
    "            \n",
    "            predictions.append(generated_caption)\n",
    "            references.append(sample[\"text\"])\n",
    "    \n",
    "    # Calculate BLEU score using the function defined earlier\n",
    "    bleu_score_result = bleu_score(predictions, references)\n",
    "    return bleu_score_result\n",
    "\n",
    "def generate_caption(caption_model, image_embedding, tokenizer, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate a caption for a given image embedding.\n",
    "    \n",
    "    Args:\n",
    "        caption_model: Trained captioning model\n",
    "        image_embedding: Image embedding from CLIP model\n",
    "        tokenizer: GPT-2 tokenizer\n",
    "        max_length: Maximum caption length\n",
    "        temperature: Sampling temperature\n",
    "    \n",
    "    Returns:\n",
    "        Generated caption as string\n",
    "    \"\"\"\n",
    "    caption_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Start with just the image embedding\n",
    "        generated_ids = []\n",
    "        \n",
    "        # Get dummy tokens for prefix\n",
    "        dummy_tokens = caption_model.get_dummy_token(1, device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Prepare input tokens\n",
    "            if len(generated_ids) == 0:\n",
    "                input_tokens = dummy_tokens\n",
    "            else:\n",
    "                input_tokens = torch.tensor([generated_ids], dtype=torch.long, device=device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = caption_model(input_tokens, image_embedding)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get next token probabilities\n",
    "            next_token_logits = logits[0, -1, :] / temperature\n",
    "            next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(next_token_probs, 1).item()\n",
    "            \n",
    "            # Check for end token\n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "            generated_ids.append(next_token)\n",
    "        \n",
    "        # Decode the generated caption\n",
    "        caption = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_caption_model_on_coco(clip_model, data_dir=\"data\", max_samples=1000, \n",
    "                                batch_size=8, num_epochs=5, split=\"train2017\"):\n",
    "    \"\"\"\n",
    "    Complete pipeline to train a captioning model on COCO dataset.\n",
    "    \n",
    "    Args:\n",
    "        clip_model: Pre-trained CLIP model\n",
    "        data_dir: Directory containing COCO dataset\n",
    "        max_samples: Maximum number of training samples\n",
    "        batch_size: Training batch size\n",
    "        num_epochs: Number of training epochs\n",
    "        split: COCO split to use (\"train2017\" or \"val2017\")\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (trained_model, training_losses, bleu_scores)\n",
    "    \"\"\"\n",
    "    # Load COCO dataset\n",
    "    print(\"Loading COCO dataset...\")\n",
    "    data_samples = DatasetLoader.load_coco_dataset(\n",
    "        data_dir=data_dir,\n",
    "        split=split,\n",
    "        max_samples=max_samples\n",
    "    )\n",
    "    \n",
    "    print(f\"Loaded {len(data_samples)} samples\")\n",
    "    \n",
    "    # Split data into train and validation\n",
    "    train_size = int(0.8 * len(data_samples))\n",
    "    train_samples = data_samples[:train_size]\n",
    "    val_samples = data_samples[train_size:]\n",
    "    \n",
    "    print(f\"Training samples: {len(train_samples)}\")\n",
    "    print(f\"Validation samples: {len(val_samples)}\")\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, training_losses, bleu_scores = train_caption_model(\n",
    "        clip_model=clip_model,\n",
    "        data_samples=train_samples,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return trained_model, training_losses, bleu_scores, val_samples\n",
    "\n",
    "def plot_training_metrics(training_losses, bleu_scores):\n",
    "    \"\"\"Plot training loss and BLEU scores over epochs.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot training loss\n",
    "    ax1.plot(training_losses, 'b-', label='Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss Over Epochs')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot BLEU scores\n",
    "    ax2.plot(bleu_scores, 'r-', label='BLEU Score')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('BLEU Score')\n",
    "    ax2.set_title('BLEU Score Over Epochs')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_caption_generation(trained_model, clip_model, val_samples, num_samples=5):\n",
    "    \"\"\"\n",
    "    Test caption generation on validation samples.\n",
    "    \n",
    "    Args:\n",
    "        trained_model: Trained captioning model\n",
    "        clip_model: CLIP model\n",
    "        val_samples: Validation samples\n",
    "        num_samples: Number of samples to test\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(\"Testing caption generation:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i in range(min(num_samples, len(val_samples))):\n",
    "        sample = val_samples[i]\n",
    "        \n",
    "        # Generate image embedding\n",
    "        image_embedding = clip_model.encode_images([sample[\"image_path\"]])[0].unsqueeze(0).to(device)\n",
    "        \n",
    "        # Generate caption\n",
    "        generated_caption = generate_caption(trained_model, image_embedding, tokenizer)\n",
    "        \n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"  Ground truth: {sample['text']}\")\n",
    "        print(f\"  Generated:    {generated_caption}\")\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a7b4f",
   "metadata": {},
   "source": [
    "## Fine-tuning CLIP Captioning Model\n",
    "\n",
    "Now we'll fine-tune a captioning model using the CLIP model defined above. The fine-tuning process includes:\n",
    "\n",
    "1. **Dataset Loading**: Load COCO dataset with image-caption pairs\n",
    "2. **Model Architecture**: Use ClipCaptionModel that takes CLIP image embeddings and generates captions using GPT-2\n",
    "3. **Training Loop**: Train with cross-entropy loss on next-token prediction\n",
    "4. **BLEU Score Evaluation**: Evaluate generated captions against ground truth using BLEU score\n",
    "5. **Visualization**: Plot training metrics and test generation quality\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **Pre-computed Embeddings**: Image embeddings are computed once and cached to speed up training\n",
    "- **Batch Processing**: Efficient batching for both training and evaluation\n",
    "- **BLEU Score Tracking**: Monitor caption quality improvement during training\n",
    "- **Temperature Sampling**: Controlled text generation with adjustable creativity\n",
    "- **Train/Val Split**: Proper evaluation on held-out validation set\n",
    "\n",
    "### Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2434dc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 07:53:16,432 - INFO - Loading COCO train2017 dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CLIP Captioning Model Fine-tuning...\n",
      "CLIP Model Type: <class 'models.clipModel.CLIPModel'>\n",
      "Loading COCO dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 07:53:16,926 - INFO - Loaded 500 COCO samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 samples\n",
      "Training samples: 400\n",
      "Validation samples: 100\n",
      "Pre-computing image embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:32<00:00,  7.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training failed with error: Can't load the model for 'gpt2'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'gpt2' is the correct path to a directory containing a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n",
      "This might be due to insufficient GPU memory. Try reducing batch_size or max_samples.\n"
     ]
    }
   ],
   "source": [
    "# Example: Train a captioning model using the CLIP model defined above\n",
    "print(\"Starting CLIP Captioning Model Fine-tuning...\")\n",
    "print(f\"CLIP Model Type: {type(model)}\")\n",
    "\n",
    "# Train the captioning model\n",
    "try:\n",
    "    trained_model, losses, bleu_scores, val_samples = train_caption_model_on_coco(\n",
    "        clip_model=model,\n",
    "        data_dir=\"data\",\n",
    "        max_samples=500,  # Adjust based on your dataset size and computational resources\n",
    "        batch_size=4,     # Adjust based on GPU memory\n",
    "        num_epochs=3,     # Start with fewer epochs for testing\n",
    "        split=\"train2017\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining completed successfully!\")\n",
    "    \n",
    "    # Plot training metrics\n",
    "    print(\"\\nPlotting training metrics...\")\n",
    "    plot_training_metrics(losses, bleu_scores)\n",
    "    \n",
    "    # Test caption generation on validation samples\n",
    "    print(\"\\nTesting caption generation on validation samples...\")\n",
    "    test_caption_generation(trained_model, model, val_samples, num_samples=5)\n",
    "    \n",
    "    # Final evaluation on validation set\n",
    "    print(\"\\nFinal evaluation on validation set...\")\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    final_bleu = evaluate_captioning_model(trained_model, model, val_samples, tokenizer, max_samples=50)\n",
    "    print(f\"Final Validation BLEU Score: {final_bleu:.4f}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Dataset not found: {e}\")\n",
    "    print(\"Please ensure COCO dataset is downloaded to the 'data' directory.\")\n",
    "    print(\"You can use the download script: bash data/download_coco.sh\")\n",
    "except Exception as e:\n",
    "    print(f\"Training failed with error: {e}\")\n",
    "    print(\"This might be due to insufficient GPU memory. Try reducing batch_size or max_samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional utility functions for comprehensive evaluation\n",
    "\n",
    "def evaluate_caption_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Compute comprehensive caption evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        predictions: List of generated captions\n",
    "        references: List of ground truth captions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with various metrics\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "    from collections import Counter\n",
    "    import re\n",
    "    \n",
    "    # BLEU scores with different n-grams\n",
    "    bleu_1_scores = []\n",
    "    bleu_2_scores = []\n",
    "    bleu_3_scores = []\n",
    "    bleu_4_scores = []\n",
    "    \n",
    "    # Other metrics\n",
    "    exact_matches = 0\n",
    "    token_overlaps = []\n",
    "    \n",
    "    smoothing = SmoothingFunction().method1\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        pred_tokens = pred.lower().split()\n",
    "        ref_tokens = ref.lower().split()\n",
    "        \n",
    "        # BLEU scores\n",
    "        bleu_1 = sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "        bleu_2 = sentence_bleu([ref_tokens], pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "        bleu_3 = sentence_bleu([ref_tokens], pred_tokens, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "        bleu_4 = sentence_bleu([ref_tokens], pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "        \n",
    "        bleu_1_scores.append(bleu_1)\n",
    "        bleu_2_scores.append(bleu_2)\n",
    "        bleu_3_scores.append(bleu_3)\n",
    "        bleu_4_scores.append(bleu_4)\n",
    "        \n",
    "        # Exact match\n",
    "        if pred.lower().strip() == ref.lower().strip():\n",
    "            exact_matches += 1\n",
    "        \n",
    "        # Token overlap (Jaccard similarity)\n",
    "        pred_set = set(pred_tokens)\n",
    "        ref_set = set(ref_tokens)\n",
    "        if len(pred_set.union(ref_set)) > 0:\n",
    "            overlap = len(pred_set.intersection(ref_set)) / len(pred_set.union(ref_set))\n",
    "            token_overlaps.append(overlap)\n",
    "    \n",
    "    return {\n",
    "        'bleu_1': np.mean(bleu_1_scores),\n",
    "        'bleu_2': np.mean(bleu_2_scores),\n",
    "        'bleu_3': np.mean(bleu_3_scores),\n",
    "        'bleu_4': np.mean(bleu_4_scores),\n",
    "        'exact_match_rate': exact_matches / len(predictions),\n",
    "        'token_overlap': np.mean(token_overlaps),\n",
    "        'num_samples': len(predictions)\n",
    "    }\n",
    "\n",
    "def save_training_results(trained_model, losses, bleu_scores, val_samples, \n",
    "                         model_path=\"clip_caption_model.pt\", results_path=\"training_results.json\"):\n",
    "    \"\"\"\n",
    "    Save training results and model for later use.\n",
    "    \n",
    "    Args:\n",
    "        trained_model: Trained captioning model\n",
    "        losses: Training losses\n",
    "        bleu_scores: BLEU scores during training\n",
    "        val_samples: Validation samples\n",
    "        model_path: Path to save model\n",
    "        results_path: Path to save training results\n",
    "    \"\"\"\n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), model_path)\n",
    "    \n",
    "    # Save training metrics\n",
    "    results = {\n",
    "        'training_losses': losses,\n",
    "        'bleu_scores': bleu_scores,\n",
    "        'final_bleu': bleu_scores[-1] if bleu_scores else 0.0,\n",
    "        'num_epochs': len(losses),\n",
    "        'validation_samples': len(val_samples)\n",
    "    }\n",
    "    \n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "def load_trained_model(model_path, clip_embed_size):\n",
    "    \"\"\"\n",
    "    Load a previously trained captioning model.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to saved model\n",
    "        clip_embed_size: Size of CLIP embeddings\n",
    "    \n",
    "    Returns:\n",
    "        Loaded captioning model\n",
    "    \"\"\"\n",
    "    model = ClipCaptionModel(\n",
    "        prefix_length=10,\n",
    "        clip_length=10,\n",
    "        prefix_size=clip_embed_size\n",
    "    ).to(device)\n",
    "    \n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    return model\n",
    "\n",
    "# Function to run comprehensive evaluation\n",
    "def run_comprehensive_evaluation(trained_model, clip_model, val_samples, max_samples=100):\n",
    "    \"\"\"\n",
    "    Run a comprehensive evaluation of the trained captioning model.\n",
    "    \n",
    "    Args:\n",
    "        trained_model: Trained captioning model\n",
    "        clip_model: CLIP model for embeddings\n",
    "        val_samples: Validation samples\n",
    "        max_samples: Maximum samples to evaluate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comprehensive metrics\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"Running comprehensive evaluation on {min(max_samples, len(val_samples))} samples...\")\n",
    "    \n",
    "    eval_samples = val_samples[:max_samples]\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    trained_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(eval_samples, desc=\"Generating captions\"):\n",
    "            image_embedding = clip_model.encode_images([sample[\"image_path\"]])[0].unsqueeze(0).to(device)\n",
    "            generated_caption = generate_caption(trained_model, image_embedding, tokenizer)\n",
    "            \n",
    "            predictions.append(generated_caption)\n",
    "            references.append(sample[\"text\"])\n",
    "    \n",
    "    # Compute comprehensive metrics\n",
    "    metrics = evaluate_caption_metrics(predictions, references)\n",
    "    \n",
    "    print(\"\\nComprehensive Evaluation Results:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"BLEU-1: {metrics['bleu_1']:.4f}\")\n",
    "    print(f\"BLEU-2: {metrics['bleu_2']:.4f}\")\n",
    "    print(f\"BLEU-3: {metrics['bleu_3']:.4f}\")\n",
    "    print(f\"BLEU-4: {metrics['bleu_4']:.4f}\")\n",
    "    print(f\"Exact Match Rate: {metrics['exact_match_rate']:.4f}\")\n",
    "    print(f\"Token Overlap: {metrics['token_overlap']:.4f}\")\n",
    "    print(f\"Number of Samples: {metrics['num_samples']}\")\n",
    "    \n",
    "    return metrics, predictions, references"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CardElephant (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
